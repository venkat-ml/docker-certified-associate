{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my DCA preparation","title":"Home"},{"location":"to_read/","text":"My Practice https://ibm.github.io/kubernetes-networking/ https://ibm.github.io/kubernetes-storage/ https://github.com/GoogleCloudPlatform/microservices-demo https://github.com/IBM/spring-boot-microservices-on-kubernetes https://github.com/andifalk/cloud-native-microservices-security https://github.com/garystafford/k8s-istio-observe-backend","title":"To read"},{"location":"Basics/","text":"Default logging driver : json-file The following ports must be available. On some systems, these ports are open by default. TCP port 2377 for cluster management communications TCP and UDP port 7946 for communication among nodes UDP port 4789 for overlay network traffic Secrets can be used by Swarm services, not standalone containers. Enable live restore There are two ways to enable the live restore setting to keep containers alive when the daemon becomes unavailable. Only do one of the following. Add the configuration to the daemon configuration file. On Linux, this defaults to /etc/docker/daemon.json. On Docker Desktop for Mac or Docker Desktop for Windows, select the Docker icon from the task bar, then click Preferences -> Daemon -> Advanced. Use the following JSON to enable live-restore. { \"live-restore\": true } Restart the Docker daemon. On Linux, you can avoid a restart (and avoid any downtime for your containers) by reloading the Docker daemon. If you use systemd, then use the command systemctl reload docker. Otherwise, send a SIGHUP signal to the dockerd process. sudo kill -SIGHUP $(pid_of_dockerd) If you prefer, you can start the dockerd process manually with the --live-restore flag. This approach is not recommended because it does not set up the environment that systemd or another process manager would use when starting the Docker process. This can cause unexpected behavior. A grant is made up of subject, role and resource set. Grants define which users can access what resources in what way. Grants are effectively ACLs, and when grouped together, they provide comprehensive access policies for an entire organisation. Only admin can manage grants, subjects, roles and access to resources. Secrets are stored at /run/secrets Default location of config file is / docker.sock owner is root and default permissions are 660 (rw-rw-----) docker run -m -> to limit the memory --privileged flag is used to enable all kernel capabilities, which enables a process running within a Docker container to bypass most of the controls such as kernel namespaces and cgroups limitations. Blocklevel storage drivers - devicemapper, btrfs, zfs Process spawned by Docker are run using root user. Secrets are encrypted at rest Config is transmitted encrypted to the manager. Tmpfs mount are available for linux. loop-lvm - testing, direct-lvm - production Swarm, UCP, DTR - order of backup and recovery Can\u2019t back metrics and monitoring data A task is a one-directional mechanism. This means that it progresses monotonically through a series of states: assigned, prepared, running, etc. If the task fails the orchestrator removes the task and its container and then creates a new task to replace it according to the desired state specified by the service. There are different kinds of resources for creating Pods: Use a Deployment, ReplicaSet or StatefulSet for Pods that are not expected to terminate, for example, web servers. Use a Job for Pods that are expected to terminate once their work is complete; for example, batch computations. Jobs are appropriate only for Pods with restartPolicy equal to OnFailure or Never. Use a DaemonSet for Pods that need to run one per eligible node. Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling. To configure the restart policy for a container, use the --restart flag when using the docker run command. The value of the --restart flag can be any of the following: no - Do not automatically restart the container. (the default) on-failure - Restart the container if it exits due to an error, which manifests as a non-zero exit code. always - Always restart the container if it stops. If it is manually stopped, it is restarted only when Docker daemon restarts or the container itself is manually restarted. (See the second bullet listed in restart policy details) unless-stopped - Similar to always, except that when the container is stopped (manually or otherwise), it is not restarted even after Docker daemon restarts. https://docs.docker.com/config/containers/start-containers-automatically/ A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image. Users can create secrets and the system also creates some secrets. To use a secret, a Pod needs to reference the secret. A secret can be used with a Pod in three ways: - As files in a volume mounted on one or more of its containers. - As container environment variable. - By the kubelet when pulling images for the Pod. By default, a container has no resource constraints and can use as much of a given resource as the host\u2019s kernel scheduler allows. Docker provides ways to control how much memory, or CPU a container can use, setting runtime configuration flags of the docker run command. Best practices for writing Dockerfiles: Create ephemeral containers Understand build context Pipe Dockerfile through stdin Exclude with .dockerignore Use multi-stage builds Don\u2019t install unnecessary packages Decouple applications Minimize the number of layers Sort multi-line arguments Leverage build cache The default filename is Dockerfile (without an extension), and using the default can make various tasks easier while working with containers. CMD instruction allows you to set a default command, which will be executed only when you run container without specifying a command. If Docker container runs with a command, the default command will be ignored. If Dockerfile has more than one CMD instruction, all but last CMD instructions are ignored. The main purpose of a CMD is to provide defaults for an executing container. ENTRYPOINT instruction allows you to configure a container that will run as an executable. It looks similar to CMD, because it also allows you to specify a command with parameters. The difference is ENTRYPOINT command and parameters are not ignored when Docker container runs with command line parameters. (There is a way to ignore ENTTRYPOINT, but it is unlikely that you will do it.) The ENV instruction sets the environment variable to the value . This value will be in the environment for all subsequent instructions in the build stage. Also, the environment variables set using ENV will persist when a container is run from the resulting image. You can view the values using docker inspect, and change them using docker run --env = . The ADD instruction operates similarly to the COPY instruction with two important differences. The ADD instruction will additionally support: Fetch remote source files if a URL is specified Auto-extraction of archive files. Extract the files of any source that are determined to be of archive file type. The EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. You can specify whether the port listens on TCP or UDP, and the default is TCP if the protocol is not specified. The EXPOSE instruction does not actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published. To actually publish the port when running the container, use the -p flag on docker run to publish and map one or more ports, or the -P flag to publish all exposed ports and map them to high-order ports. The RUN instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile. CMD instruction allows you to set a default command, which will be executed only when you run container without specifying a command. If Docker container runs with a command, the default command will be ignored. If Dockerfile has more than one CMD instruction, all but last CMD instructions are ignored. Note: Don\u2019t confuse RUN with CMD. RUN actually runs a command and commits the result; CMD does not execute anything at build time, but specifies the intended command for the image. When building an image, do not use your root directory, /, as the PATH as it causes the build to transfer the entire contents of your hard drive to the Docker daemon. The build is run by the Docker daemon, not by the CLI. The first thing a build process does is send the entire context (recursively) to the daemon. Build context : All the files found in the specific path or URL To create a tag TARGET_IMAGE that refers to SOURCE_IMAGE use: docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG] Tag an image referenced by ID To tag a local image with ID \u201c0e5574283393\u201d into the \u201cfedora\u201d repository with \u201cversion1.0\u201d: $ docker tag 0e5574283393 fedora/httpd:version1.0 Tag an image referenced by Name To tag a local image with name \u201chttpd\u201d into the \u201cfedora\u201d repository with \u201cversion1.0\u201d: $ docker tag httpd fedora/httpd:version1.0 Note that since the tag name is not specified, the alias is created for an existing local version httpd:latest. Tag an image referenced by Name and Tag To tag a local image with name \u201chttpd\u201d and tag \u201ctest\u201d into the \u201cfedora\u201d repository with \u201cversion1.0.test\u201d: $ docker tag httpd:test fedora/httpd:version1.0.test To show untagged images, or dangling, use: docker images --filter \"dangling=true\" This will display untagged images that are the leaves of the images tree (not intermediary layers). These images occur when a new build of an image takes the repo:tag away from the image ID, leaving it as : or untagged. A warning will be issued if trying to remove an image when a container is presently using it. By having this flag it allows for batch cleanup. DTR - Immutable tags By default, users with read and write access to a repository can push the same tag multiple times to that repository. For example, when user A pushes an image to library/wordpress:latest, there is no preventing user B from pushing an image with the same name but a completely different functionality. This can make it difficult to trace the image back to the build that generated it. To prevent tags from being overwritten, you can configure a repository to be immutable in the DTR web UI. Once configured, DTR will not allow anyone else to push another image tag with the same name. Healthcheck The HEALTHCHECK instruction tells Docker how to test a container to check that it is still working. This can detect cases such as a web server that is stuck in an infinite loop and unable to handle new connections, even though the server process is still running. The HEALTHCHECK instruction tells Docker how to test a container to check that it is still working. The HEALTHCHECK instruction has two forms: HEALTHCHECK [OPTIONS] CMD command (check container health by running a command inside the container) HEALTHCHECK NONE (disable any healthcheck inherited from the base image) When a container has a healthcheck specified, it has a health status in addition to its normal status. The command after the CMD keyword can be either a shell command (e.g. HEALTHCHECK CMD /bin/check-running) or an exec array (as with other Dockerfile commands; see e.g. ENTRYPOINT for details) ARG vs ENV The ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg = flag. https://docs.docker.com/engine/reference/builder/#arg Unlike an ARG instruction, ENV values are always persisted in the built image. The variable expansion technique in this example allows you to pass arguments from the command line and persist them in the final image by leveraging the ENV instruction. You can use an ARG or an ENV instruction to specify variables that are available to the RUN instruction. Environment variables defined using the ENV instruction always override an ARG instruction of the same name. https://docs.docker.com/engine/reference/builder/#run#using-arg-variables LABEL LABEL <key>=<value> <key>=<value> <key>=<value> ... The LABEL instruction adds metadata to an image. A LABEL is a key-value pair. Example: LABEL version=\"1.0\" docker search To search the Docker Hub for images use: docker search [OPTIONS] TERM The filtering flag (-f or --filter) format is a key=value pair. If there is more than one filter, then pass multiple flags (e.g. --filter is-automated=true --filter stars=3) The currently supported filters are: stars (int - number of stars the image has) is-automated (boolean - true or false) - is the image automated or not is-official (boolean - true or false) - is the image official or not https://docs.docker.com/engine/reference/commandline/search/#filtering RUN The RUN instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile. Layering RUN instructions and generating commits conforms to the core concepts of Docker where commits are cheap and containers can be created from any point in an image\u2019s history, much like source control. Therefore, it\u2019s ok to have more than one RUN instruction. https://docs.docker.com/engine/reference/builder/#run docker save To save one or more images to a tar archive (streamed to STDOUT by default) use: docker save [OPTIONS] IMAGE [IMAGE...] The achieve can be distributed through different channels such as: central file server, version-control system, sent it to you over email or shared it via flash drive. https://docs.docker.com/engine/reference/commandline/save/ Docker provides a command to load images into Docker from a file. With this tool, you can load images that you acquired through other channels. https://docs.docker.com/engine/reference/commandline/image_load/ Multi stage builds Multi-stage builds allow you to drastically reduce the size of your final image, without struggling to reduce the number of intermediate layers and files. With multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don\u2019t want in the final image. https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds Container vs Image Fundamentally, a container is nothing but a running process, with some added encapsulation features applied to it in order to keep it isolated from the host and from other containers. An image includes everything needed to run an application - the code or binary, runtimes, dependencies, and any other filesystem objects required. Info vs Inspect docker info - system wide information docker inspect <container> /etc/docker/daemon.json To make the modifications and send the HUP signal in one-line use: echo '{\"debug\": true}' > /etc/docker/daemon.json ; sudo kill -HUP /var/run/docker.sock UNIX socket that Docker daemon is listening to. It's the main entry point for Docker API. It also can be TCP socket but by default for security reasons Docker defaults to use UNIX socket. privileged vs unprivileged containers By default, Docker containers are \u201cunprivileged\u201d and cannot, for example, run a Docker daemon inside a Docker container. This is because by default a container is not allowed to access any devices, but a \u201cprivileged\u201d container is given access to all devices (see the documentation on cgroups devices). When the operator executes docker run --privileged, Docker will enable access to all devices on the host as well as set some configuration in AppArmor or SELinux to allow the container nearly all the same access to the host as processes running outside containers on the host. Additional information about running with --privileged is available on the Docker Blog. If you want to limit access to a specific device or devices you can use the --device flag. It allows you to specify one or more devices that will be accessible within the container. $ docker run --device=/dev/snd:/dev/snd ... Following the least privileged principle, you should use --device instead of --privileged. https://docs.docker.com/engine/reference/run/#/runtime-privilege-and-linux-capabilities Docker Engine has a default list of capabilities for all newly created containers. Syslog Having a central location for all Engine and container logs is recommended. This provides \"off-node\" access to all the logs, empowering developers without having to grant them SSH access. To enable centralized logging, modify /etc/docker/daemon.json and add the following: { \"log-level\": \"syslog\", \"log-opts\": {syslog-address=tcp://192.x.x.x} } Then restart the daemon: sudo systemctl restart docker certificate authentication You don\u2019t need to run the docker client with sudo or the docker group when you use certificate authentication. That means anyone with the keys can give any instructions to your Docker daemon, giving them root access to the machine hosting the daemon. However, guard these keys as you would a root password! Copy-on-write strategy When an existing file in a container is modified, the storage driver performs a copy-on-write operation. The specifics steps involved depend on the specific storage driver. For the aufs, overlay, and overlay2 drivers, the copy-on-write operation follows this rough sequence: Search through the image layers for the file to update. The process starts at the newest layer and works down to the base layer one layer at a time. When results are found, they are added to a cache to speed future operations. Perform a copy_up operation on the first copy of the file that is found, to copy the file to the container\u2019s writable layer. Any modifications are made to this copy of the file, and the container cannot see the read-only copy of the file that exists in the lower layer. Image A Docker image is built up from a series of layers. Each layer represents an instruction in the image\u2019s Dockerfile. Each layer except the very last one is read-only. Each layer is only a set of differences from the layer before it. Docker namespaces Docker namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is limited to that namespace. Docker Engine uses namespaces such as the following on Linux: - The pid namespace: Process isolation (PID: Process ID). - The net namespace: Managing network interfaces (NET: Networking). - The ipc namespace: Managing access to IPC resources (IPC: InterProcess Communication). - The mnt namespace: Managing filesystem mount points (MNT: Mount). - The uts namespace: Isolating kernel and version identifiers. (UTS: Unix Timesharing System). logging mechanism Docker includes multiple logging mechanisms to help you get information from running containers and services. These mechanisms are called logging drivers. To find the current logging driver for a running container, run the docker inspect command, substituting the container name or ID for : $ docker inspect <CONTAINER> cpuset-cpus By default, each container\u2019s access to the host machine\u2019s CPU cycles is unlimited. You can set various constraints to limit a given container\u2019s access to the host machine\u2019s CPU cycles. Several runtime flags allow you to configure the amount of access to CPU resources your container has. When you use these settings, Docker modifies the settings for the container\u2019s cgroup on the host machine. --cpuset-cpus Limit the specific CPUs or cores a container can use. A comma-separated list or hyphen-separated range of CPUs a container can use, if you have more than one CPU. The first CPU is numbered 0. A valid value might be 0-3 (to use the first, second, third, and fourth CPU) or 1,3 (to use the second and fourth CPU). https://docs.docker.com/config/containers/resource_constraints/#cpu We can set cpus in which to allow execution for containers. Examples: $ docker run -it --cpuset-cpus=\"1,3\" ubuntu:14.04 /bin/bash This means processes in container can be executed on cpu 1 and cpu 3. $ docker run -it --cpuset-cpus=\"0-2\" ubuntu:14.04 /bin/bash This means processes in container can be executed on cpu 0, cpu 1 and cpu 2. https://docs.docker.com/engine/reference/run/#cpuset-constraint docker system events Use docker system events to get real-time events from the server. These events differ per Docker object type such as containers, images, plugins, volumes and daemons. enable/disable docker Most current Linux distributions (RHEL, CentOS, Fedora, Ubuntu 16.04 and higher) use systemd to manage which services start when the system boots. Ubuntu 14.10 and below use upstart. $ sudo systemctl enable docker To disable this behavior, use disable instead. $ sudo systemctl disable docker Control groups Docker Engine on Linux also relies on another technology called control groups (cgroups). A cgroup limits an application to a specific set of resources. Control groups allow Docker Engine to share available hardware resources to containers and optionally enforce limits and constraints. For example, you can limit the memory or CPU available to a specific container. --cpus= Specify how much of the available CPU resources a container can use. For instance, if the host machine has two CPUs and you set --cpus=\"1.5\", the container is guaranteed at most one and a half of the CPUs. This is the equivalent of setting --cpu-period=\"100000\" and --cpu-quota=\"150000\". Available in Docker 1.13 and higher. -m or --memory= The maximum amount of memory the container can use. If you set this option, the minimum allowed value is 4m (4 megabyte). Most of these options take a positive integer, followed by a suffix of b, k, m, g, to indicate bytes, kilobytes, megabytes, or gigabytes. log driver modes Docker provides two modes for delivering messages from the container to the log driver: (default) direct, blocking delivery from container to driver non-blocking delivery that stores log messages in an intermediate per-container ring buffer for consumption by driver --default-ulimit --default-ulimit allows you to set the default ulimit options to use for all containers. It takes the same options as --ulimit for docker run. If these defaults are not set, ulimit settings will be inherited, if not set on docker run, from the Docker daemon. Any --ulimit options passed to docker run will overwrite these defaults. Be careful setting nproc with the ulimit flag as nproc is designed by Linux to set the maximum number of processes available to a user, not to a container. For details please check the run reference. Secure registry access images stored on a secure registry that is using a self-signed certificate configure our registry as \"insecure\" in Docker Engine's daemon.json file. to trust self-signed certificates. We will add DTR's created Certificate Authority (CA) into our system's trusted-CA list. docker image import docker image import will only retrieve image layers containing binaries, libraries, and configurations for the process but without any meta-information about how to launch the process, what volumes to use, what ports should be used, and so on. How do we only list containers created from an alpine:3.10 image? a) docker ps --format ancestor=alpine:3.10 b) docker container ls --filter ancestor=alpine:3.10 Which of the following is true about privileged containers? a) Resource limits will be avoided (CPU, memory, and disk I/O). b) These containers run with all available capabilities. docker logs The docker logs command batch-retrieves logs present at the time of execution. Note This command is only functional for containers that are started with the json-file or journald logging driver. docker container logs vs docker logs The docker logs command batch-retrieves logs present at the time of execution. This command is only functional for containers that are started with the json-file or journald logging driver. docker container logs - Fetch the logs of a container docker run In the Dockerfile context, The RUN instruction will execute any commands in a new layer on top of the current image and commit the results. Layering RUN instructions and generating commits conforms to the core concepts of Docker where commits are cheap and containers can be created from any point in an image\u2019s history, much like source control. https://docs.docker.com/engine/reference/builder/#run Note: Don\u2019t confuse RUN with CMD. RUN actually runs a command and commits the result; CMD does not execute anything at build time, but specifies the intended command for the image. How can we review ports published for a container named webserver? a) Using docker container ls --filter name=webserver. b) Using docker container port webserver. c) Using docker container inspect webserver --format=\"{{ .NetworkSettings.Ports }}\". docker export docker export command exports a container\u2019s filesystem as a tar archive. docker rmi with multiple tags If an image has one or more tags referencing it, you must remove all of them before the image is removed or you can use the -f flag and specify the image\u2019s short or long ID, then this command untags and removes all images that match the specified ID. docker rmi -f fd484f19954f docker update The docker update command dynamically updates container configuration. You can use this command to prevent containers from consuming too many resources from their Docker host. With a single command, you can place limits on a single container or on many. To specify more than one container, provide space-separated list of container names or IDs. Find the cgroup for a given container For each container, one cgroup is created in each hierarchy. On older systems with older versions of the LXC userland tools, the name of the cgroup is the name of the container. With more recent versions of the LXC tools, the cgroup is lxc/ . Pause container - An empty container which holds cgroups, reservations, namespaces of a pod before its individual container is created. Builder Pattern - It is a design pattern used to maintain 2 individual Dockerfiles for app development and production purposes. Configuration files\ud83d\udd17 By default, the Docker command line stores its configuration files in a directory called .docker within your $HOME directory. Antivirus software and Docker When antivirus software scans files used by Docker, these files may be locked in a way that causes Docker commands to hang. One way to reduce these problems is to add the Docker data directory (/var/lib/docker on Linux, %ProgramData%\\docker on Windows Server, or $HOME/Library/Containers/com.docker.docker/ on Mac) to the antivirus\u2019s exclusion list. However, this comes with the trade-off that viruses or malware in Docker images, writable layers of containers, or volumes are not detected. If you do choose to exclude Docker\u2019s data directory from background virus scanning, you may want to schedule a recurring task that stops Docker, scans the data directory, and restarts Docker. Multistage Builds $ docker build -t alexellis2/href-counter:latest . $ docker build --target builder -t alexellis2/href-counter:latest . The container is assigned a separate IP address for every Docker network it connects to. The IP address is assigned from the pool assigned to the network, so the Docker daemon effectively acts as a DHCP server for each container. com.docker.network.bridge.host_binding_ipv4 --ip Default IP when binding container ports The default value of the delay between two reset occurrences of a container - 100ms This command is only functional for containers that are started with the json-file or journald logging driver. (default logging drivers) docker service create --update-parallelism Maximum number of tasks updated simultaneously (0 to update all at once) metrics CPU metrics: cpuacct.stat Memory metrics: memory.stat View task state Run docker service ps to get the state of a task. The CURRENT STATE field shows the task\u2019s state and how long it\u2019s been there. $ docker service ps webserver","title":"Concepts"},{"location":"Basics/#to-create-a-tag-target_image-that-refers-to-source_image-use","text":"docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG] Tag an image referenced by ID To tag a local image with ID \u201c0e5574283393\u201d into the \u201cfedora\u201d repository with \u201cversion1.0\u201d: $ docker tag 0e5574283393 fedora/httpd:version1.0 Tag an image referenced by Name To tag a local image with name \u201chttpd\u201d into the \u201cfedora\u201d repository with \u201cversion1.0\u201d: $ docker tag httpd fedora/httpd:version1.0 Note that since the tag name is not specified, the alias is created for an existing local version httpd:latest. Tag an image referenced by Name and Tag To tag a local image with name \u201chttpd\u201d and tag \u201ctest\u201d into the \u201cfedora\u201d repository with \u201cversion1.0.test\u201d: $ docker tag httpd:test fedora/httpd:version1.0.test To show untagged images, or dangling, use: docker images --filter \"dangling=true\" This will display untagged images that are the leaves of the images tree (not intermediary layers). These images occur when a new build of an image takes the repo:tag away from the image ID, leaving it as : or untagged. A warning will be issued if trying to remove an image when a container is presently using it. By having this flag it allows for batch cleanup.","title":"To create a tag TARGET_IMAGE that refers to SOURCE_IMAGE use:"},{"location":"Basics/#dtr-immutable-tags","text":"By default, users with read and write access to a repository can push the same tag multiple times to that repository. For example, when user A pushes an image to library/wordpress:latest, there is no preventing user B from pushing an image with the same name but a completely different functionality. This can make it difficult to trace the image back to the build that generated it. To prevent tags from being overwritten, you can configure a repository to be immutable in the DTR web UI. Once configured, DTR will not allow anyone else to push another image tag with the same name.","title":"DTR - Immutable tags"},{"location":"Basics/#healthcheck","text":"The HEALTHCHECK instruction tells Docker how to test a container to check that it is still working. This can detect cases such as a web server that is stuck in an infinite loop and unable to handle new connections, even though the server process is still running. The HEALTHCHECK instruction tells Docker how to test a container to check that it is still working. The HEALTHCHECK instruction has two forms: HEALTHCHECK [OPTIONS] CMD command (check container health by running a command inside the container) HEALTHCHECK NONE (disable any healthcheck inherited from the base image) When a container has a healthcheck specified, it has a health status in addition to its normal status. The command after the CMD keyword can be either a shell command (e.g. HEALTHCHECK CMD /bin/check-running) or an exec array (as with other Dockerfile commands; see e.g. ENTRYPOINT for details)","title":"Healthcheck"},{"location":"Basics/#arg-vs-env","text":"The ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg = flag. https://docs.docker.com/engine/reference/builder/#arg Unlike an ARG instruction, ENV values are always persisted in the built image. The variable expansion technique in this example allows you to pass arguments from the command line and persist them in the final image by leveraging the ENV instruction. You can use an ARG or an ENV instruction to specify variables that are available to the RUN instruction. Environment variables defined using the ENV instruction always override an ARG instruction of the same name. https://docs.docker.com/engine/reference/builder/#run#using-arg-variables","title":"ARG vs ENV"},{"location":"Basics/#label","text":"LABEL <key>=<value> <key>=<value> <key>=<value> ... The LABEL instruction adds metadata to an image. A LABEL is a key-value pair. Example: LABEL version=\"1.0\"","title":"LABEL"},{"location":"Basics/#docker-search","text":"To search the Docker Hub for images use: docker search [OPTIONS] TERM The filtering flag (-f or --filter) format is a key=value pair. If there is more than one filter, then pass multiple flags (e.g. --filter is-automated=true --filter stars=3) The currently supported filters are: stars (int - number of stars the image has) is-automated (boolean - true or false) - is the image automated or not is-official (boolean - true or false) - is the image official or not https://docs.docker.com/engine/reference/commandline/search/#filtering","title":"docker search"},{"location":"Basics/#run","text":"The RUN instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile. Layering RUN instructions and generating commits conforms to the core concepts of Docker where commits are cheap and containers can be created from any point in an image\u2019s history, much like source control. Therefore, it\u2019s ok to have more than one RUN instruction. https://docs.docker.com/engine/reference/builder/#run","title":"RUN"},{"location":"Basics/#docker-save","text":"To save one or more images to a tar archive (streamed to STDOUT by default) use: docker save [OPTIONS] IMAGE [IMAGE...] The achieve can be distributed through different channels such as: central file server, version-control system, sent it to you over email or shared it via flash drive. https://docs.docker.com/engine/reference/commandline/save/ Docker provides a command to load images into Docker from a file. With this tool, you can load images that you acquired through other channels. https://docs.docker.com/engine/reference/commandline/image_load/","title":"docker save"},{"location":"Basics/#multi-stage-builds","text":"Multi-stage builds allow you to drastically reduce the size of your final image, without struggling to reduce the number of intermediate layers and files. With multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don\u2019t want in the final image. https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds","title":"Multi stage builds"},{"location":"Basics/#container-vs-image","text":"Fundamentally, a container is nothing but a running process, with some added encapsulation features applied to it in order to keep it isolated from the host and from other containers. An image includes everything needed to run an application - the code or binary, runtimes, dependencies, and any other filesystem objects required.","title":"Container vs Image"},{"location":"Basics/#info-vs-inspect","text":"docker info - system wide information docker inspect <container>","title":"Info vs Inspect"},{"location":"Basics/#etcdockerdaemonjson","text":"To make the modifications and send the HUP signal in one-line use: echo '{\"debug\": true}' > /etc/docker/daemon.json ; sudo kill -HUP","title":"/etc/docker/daemon.json"},{"location":"Basics/#varrundockersock","text":"UNIX socket that Docker daemon is listening to. It's the main entry point for Docker API. It also can be TCP socket but by default for security reasons Docker defaults to use UNIX socket.","title":"/var/run/docker.sock"},{"location":"Basics/#privileged-vs-unprivileged-containers","text":"By default, Docker containers are \u201cunprivileged\u201d and cannot, for example, run a Docker daemon inside a Docker container. This is because by default a container is not allowed to access any devices, but a \u201cprivileged\u201d container is given access to all devices (see the documentation on cgroups devices). When the operator executes docker run --privileged, Docker will enable access to all devices on the host as well as set some configuration in AppArmor or SELinux to allow the container nearly all the same access to the host as processes running outside containers on the host. Additional information about running with --privileged is available on the Docker Blog. If you want to limit access to a specific device or devices you can use the --device flag. It allows you to specify one or more devices that will be accessible within the container. $ docker run --device=/dev/snd:/dev/snd ... Following the least privileged principle, you should use --device instead of --privileged. https://docs.docker.com/engine/reference/run/#/runtime-privilege-and-linux-capabilities Docker Engine has a default list of capabilities for all newly created containers.","title":"privileged vs unprivileged containers"},{"location":"Basics/#syslog","text":"Having a central location for all Engine and container logs is recommended. This provides \"off-node\" access to all the logs, empowering developers without having to grant them SSH access. To enable centralized logging, modify /etc/docker/daemon.json and add the following: { \"log-level\": \"syslog\", \"log-opts\": {syslog-address=tcp://192.x.x.x} } Then restart the daemon: sudo systemctl restart docker","title":"Syslog"},{"location":"Basics/#certificate-authentication","text":"You don\u2019t need to run the docker client with sudo or the docker group when you use certificate authentication. That means anyone with the keys can give any instructions to your Docker daemon, giving them root access to the machine hosting the daemon. However, guard these keys as you would a root password!","title":"certificate authentication"},{"location":"Basics/#copy-on-write-strategy","text":"When an existing file in a container is modified, the storage driver performs a copy-on-write operation. The specifics steps involved depend on the specific storage driver. For the aufs, overlay, and overlay2 drivers, the copy-on-write operation follows this rough sequence: Search through the image layers for the file to update. The process starts at the newest layer and works down to the base layer one layer at a time. When results are found, they are added to a cache to speed future operations. Perform a copy_up operation on the first copy of the file that is found, to copy the file to the container\u2019s writable layer. Any modifications are made to this copy of the file, and the container cannot see the read-only copy of the file that exists in the lower layer.","title":"Copy-on-write strategy"},{"location":"Basics/#image","text":"A Docker image is built up from a series of layers. Each layer represents an instruction in the image\u2019s Dockerfile. Each layer except the very last one is read-only. Each layer is only a set of differences from the layer before it.","title":"Image"},{"location":"Basics/#docker-namespaces","text":"Docker namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is limited to that namespace. Docker Engine uses namespaces such as the following on Linux: - The pid namespace: Process isolation (PID: Process ID). - The net namespace: Managing network interfaces (NET: Networking). - The ipc namespace: Managing access to IPC resources (IPC: InterProcess Communication). - The mnt namespace: Managing filesystem mount points (MNT: Mount). - The uts namespace: Isolating kernel and version identifiers. (UTS: Unix Timesharing System).","title":"Docker namespaces"},{"location":"Basics/#logging-mechanism","text":"Docker includes multiple logging mechanisms to help you get information from running containers and services. These mechanisms are called logging drivers. To find the current logging driver for a running container, run the docker inspect command, substituting the container name or ID for : $ docker inspect <CONTAINER>","title":"logging mechanism"},{"location":"Basics/#cpuset-cpus","text":"By default, each container\u2019s access to the host machine\u2019s CPU cycles is unlimited. You can set various constraints to limit a given container\u2019s access to the host machine\u2019s CPU cycles. Several runtime flags allow you to configure the amount of access to CPU resources your container has. When you use these settings, Docker modifies the settings for the container\u2019s cgroup on the host machine. --cpuset-cpus Limit the specific CPUs or cores a container can use. A comma-separated list or hyphen-separated range of CPUs a container can use, if you have more than one CPU. The first CPU is numbered 0. A valid value might be 0-3 (to use the first, second, third, and fourth CPU) or 1,3 (to use the second and fourth CPU). https://docs.docker.com/config/containers/resource_constraints/#cpu We can set cpus in which to allow execution for containers. Examples: $ docker run -it --cpuset-cpus=\"1,3\" ubuntu:14.04 /bin/bash This means processes in container can be executed on cpu 1 and cpu 3. $ docker run -it --cpuset-cpus=\"0-2\" ubuntu:14.04 /bin/bash This means processes in container can be executed on cpu 0, cpu 1 and cpu 2. https://docs.docker.com/engine/reference/run/#cpuset-constraint","title":"cpuset-cpus"},{"location":"Basics/#docker-system-events","text":"Use docker system events to get real-time events from the server. These events differ per Docker object type such as containers, images, plugins, volumes and daemons.","title":"docker system events"},{"location":"Basics/#enabledisable-docker","text":"Most current Linux distributions (RHEL, CentOS, Fedora, Ubuntu 16.04 and higher) use systemd to manage which services start when the system boots. Ubuntu 14.10 and below use upstart. $ sudo systemctl enable docker To disable this behavior, use disable instead. $ sudo systemctl disable docker","title":"enable/disable docker"},{"location":"Basics/#control-groups","text":"Docker Engine on Linux also relies on another technology called control groups (cgroups). A cgroup limits an application to a specific set of resources. Control groups allow Docker Engine to share available hardware resources to containers and optionally enforce limits and constraints. For example, you can limit the memory or CPU available to a specific container.","title":"Control groups"},{"location":"Basics/#-cpus","text":"Specify how much of the available CPU resources a container can use. For instance, if the host machine has two CPUs and you set --cpus=\"1.5\", the container is guaranteed at most one and a half of the CPUs. This is the equivalent of setting --cpu-period=\"100000\" and --cpu-quota=\"150000\". Available in Docker 1.13 and higher.","title":"--cpus="},{"location":"Basics/#-m-or-memory","text":"The maximum amount of memory the container can use. If you set this option, the minimum allowed value is 4m (4 megabyte). Most of these options take a positive integer, followed by a suffix of b, k, m, g, to indicate bytes, kilobytes, megabytes, or gigabytes.","title":"-m or --memory="},{"location":"Basics/#log-driver-modes","text":"Docker provides two modes for delivering messages from the container to the log driver: (default) direct, blocking delivery from container to driver non-blocking delivery that stores log messages in an intermediate per-container ring buffer for consumption by driver","title":"log driver modes"},{"location":"Basics/#-default-ulimit","text":"--default-ulimit allows you to set the default ulimit options to use for all containers. It takes the same options as --ulimit for docker run. If these defaults are not set, ulimit settings will be inherited, if not set on docker run, from the Docker daemon. Any --ulimit options passed to docker run will overwrite these defaults. Be careful setting nproc with the ulimit flag as nproc is designed by Linux to set the maximum number of processes available to a user, not to a container. For details please check the run reference.","title":"--default-ulimit"},{"location":"Basics/#secure-registry","text":"access images stored on a secure registry that is using a self-signed certificate configure our registry as \"insecure\" in Docker Engine's daemon.json file. to trust self-signed certificates. We will add DTR's created Certificate Authority (CA) into our system's trusted-CA list.","title":"Secure registry"},{"location":"Basics/#docker-image-import","text":"docker image import will only retrieve image layers containing binaries, libraries, and configurations for the process but without any meta-information about how to launch the process, what volumes to use, what ports should be used, and so on.","title":"docker image import"},{"location":"Basics/#how-do-we-only-list-containers-created-from-an-alpine310-image","text":"a) docker ps --format ancestor=alpine:3.10 b) docker container ls --filter ancestor=alpine:3.10","title":"How do we only list containers created from an alpine:3.10 image?"},{"location":"Basics/#which-of-the-following-is-true-about-privileged-containers","text":"a) Resource limits will be avoided (CPU, memory, and disk I/O). b) These containers run with all available capabilities.","title":"Which of the following is true about privileged containers?"},{"location":"Basics/#docker-logs","text":"The docker logs command batch-retrieves logs present at the time of execution. Note This command is only functional for containers that are started with the json-file or journald logging driver.","title":"docker logs"},{"location":"Basics/#docker-container-logs-vs-docker-logs","text":"The docker logs command batch-retrieves logs present at the time of execution. This command is only functional for containers that are started with the json-file or journald logging driver. docker container logs - Fetch the logs of a container","title":"docker container logs vs docker logs"},{"location":"Basics/#docker-run","text":"In the Dockerfile context, The RUN instruction will execute any commands in a new layer on top of the current image and commit the results. Layering RUN instructions and generating commits conforms to the core concepts of Docker where commits are cheap and containers can be created from any point in an image\u2019s history, much like source control. https://docs.docker.com/engine/reference/builder/#run Note: Don\u2019t confuse RUN with CMD. RUN actually runs a command and commits the result; CMD does not execute anything at build time, but specifies the intended command for the image.","title":"docker run"},{"location":"Basics/#how-can-we-review-ports-published-for-a-container-named-webserver","text":"a) Using docker container ls --filter name=webserver. b) Using docker container port webserver. c) Using docker container inspect webserver --format=\"{{ .NetworkSettings.Ports }}\".","title":"How can we review ports published for a container named webserver?"},{"location":"Basics/#docker-export","text":"docker export command exports a container\u2019s filesystem as a tar archive.","title":"docker export"},{"location":"Basics/#docker-rmi-with-multiple-tags","text":"If an image has one or more tags referencing it, you must remove all of them before the image is removed or you can use the -f flag and specify the image\u2019s short or long ID, then this command untags and removes all images that match the specified ID. docker rmi -f fd484f19954f","title":"docker rmi with multiple tags"},{"location":"Basics/#docker-update","text":"The docker update command dynamically updates container configuration. You can use this command to prevent containers from consuming too many resources from their Docker host. With a single command, you can place limits on a single container or on many. To specify more than one container, provide space-separated list of container names or IDs.","title":"docker update"},{"location":"Basics/#find-the-cgroup-for-a-given-container","text":"For each container, one cgroup is created in each hierarchy. On older systems with older versions of the LXC userland tools, the name of the cgroup is the name of the container. With more recent versions of the LXC tools, the cgroup is lxc/ . Pause container - An empty container which holds cgroups, reservations, namespaces of a pod before its individual container is created. Builder Pattern - It is a design pattern used to maintain 2 individual Dockerfiles for app development and production purposes.","title":"Find the cgroup for a given container"},{"location":"Basics/#configuration-files","text":"By default, the Docker command line stores its configuration files in a directory called .docker within your $HOME directory.","title":"Configuration files\ud83d\udd17"},{"location":"Basics/#antivirus-software-and-docker","text":"When antivirus software scans files used by Docker, these files may be locked in a way that causes Docker commands to hang. One way to reduce these problems is to add the Docker data directory (/var/lib/docker on Linux, %ProgramData%\\docker on Windows Server, or $HOME/Library/Containers/com.docker.docker/ on Mac) to the antivirus\u2019s exclusion list. However, this comes with the trade-off that viruses or malware in Docker images, writable layers of containers, or volumes are not detected. If you do choose to exclude Docker\u2019s data directory from background virus scanning, you may want to schedule a recurring task that stops Docker, scans the data directory, and restarts Docker.","title":"Antivirus software and Docker"},{"location":"Basics/#multistage-builds","text":"$ docker build -t alexellis2/href-counter:latest . $ docker build --target builder -t alexellis2/href-counter:latest . The container is assigned a separate IP address for every Docker network it connects to. The IP address is assigned from the pool assigned to the network, so the Docker daemon effectively acts as a DHCP server for each container. com.docker.network.bridge.host_binding_ipv4 --ip Default IP when binding container ports The default value of the delay between two reset occurrences of a container - 100ms This command is only functional for containers that are started with the json-file or journald logging driver. (default logging drivers)","title":"Multistage Builds"},{"location":"Basics/#docker-service-create","text":"--update-parallelism Maximum number of tasks updated simultaneously (0 to update all at once)","title":"docker service create"},{"location":"Basics/#metrics","text":"CPU metrics: cpuacct.stat Memory metrics: memory.stat","title":"metrics"},{"location":"Basics/#view-task-state","text":"Run docker service ps to get the state of a task. The CURRENT STATE field shows the task\u2019s state and how long it\u2019s been there. $ docker service ps webserver","title":"View task state"},{"location":"Basics/commands/","text":"docker service ps # to list the tasks docker service inspect # to inspect the service for configuration details docker service update --publish-rm 80 my_web # There is nothing like port in command line argument flags docker run -d --restart unless-stopped redis # There is nothing like policy in command line argument flags docker node update --label-add docker node update --label-rm There are no volumes for services.. We can specify mount using --mount flag. The type of mount can be either volume, bind, tmpfs, or npipe. Defaults to volume if no type is specified. docker build docker build [OPTIONS] PATH | URL | - --file , -f Name of the Dockerfile (Default is 'PATH/Dockerfile') docker service create [OPTIONS] IMAGE [COMMAND] [ARG...] --label , -l --publish , -p --tty , -t docker service scale <SERVICE-ID>=<NUMBER-OF-TASKS> Scale can only be used with replicated service docker service rollback [OPTIONS] SERVICE docker service update --rollback SERVICE docker service update --mount-add SERVICE docker service update --publish-add SERVICE docker service update --network-add SERVICE docker service update --placement-pref-add SERVICE docker service update --label-add SERVICE docker service update --update-delay 30s redis $ docker service create \\ --name my-service \\ --replicas 3 \\ --mount type=volume,source=my-volume,destination=/path/in/container \\ nginx:alpine Examples of services might include an HTTP server, a database, or any other type of executable program that you wish to run in a distributed environment. docker service create --constraint-add docker service create --constraint-rm docker service create --placement-pref-add docker service create --placement-pref-rm docker service update --publish-add docker service update --publish-rm docker service rm SERVICE [SERVICE...] docker inspect [OPTIONS] NAME|ID [NAME|ID...] Docker inspect provides detailed information on constructs controlled by Docker. By default, docker inspect will render results in a JSON array. Sometimes, such as planned maintenance times, you need to set a node to DRAIN availability. DRAIN availability prevents a node from receiving new tasks from the swarm manager. It also means the manager stops tasks running on the node and launches replica tasks on a node with ACTIVE availability. docker service create \\ --name nginx-workers-only \\ --constraint node.role==worker \\ nginx docker service scale - can scale up or down one or multiple services - can only work on replicated services docker service scale backend=3 frontend=5 docker service ls Node can only be drained for maintanence docker node update --availability drain node-1 docker node inspect self Placement prefs allow you to \"spread\" your replicas across nodes with certain tags, choosing to put the replicas as much \"diverse\" as possible. Constraints will constrain (limit) the replicas to those nodes that match the constraint. docker service create \\ --replicas 2 \\ --name webserver \\ --placement-pref 'spread=node.labels.datacenter' \\ mywebservice:production docker service create \\ --constraint 'node.labels.disk == ssd' \\ myreporter:latest docker network create -d overlay my-overlay Overlay network can be created from Swarm manager only, not from worker nodes Tip: There is nothing like --set in docker commandline arguments Tip: There is nothing like --filter in docker inspect, it is always --format Tip: There is nothing like docker xxx info.. There is only docker (system) info Tip: There is nothing like docker list Tip: There is nothing like --add, --set, --custom Tip: There is nothing like --networks or --list Tip: There is nothing like --ignore-tls DTR image deletion DTR only allows deleting images if the image has not been signed. You first need to delete all the trust data associated with the image before you are able to delete the image. HEALTHCHECK The HEALTHCHECK instruction has two forms: HEALTHCHECK [OPTIONS] CMD command (check container health by running a command inside the container) HEALTHCHECK NONE (disable any healthcheck inherited from the base image) The HEALTHCHECK instruction tells Docker how to test a container to check that it is still working. This can detect cases such as a web server that is stuck in an infinite loop and unable to handle new connections, even though the server process is still running. When a container has a healthcheck specified, it has a health status in addition to its normal status. This status is initially starting. Whenever a health check passes, it becomes healthy (whatever state it was previously in). After a certain number of consecutive failures, it becomes unhealthy. Best Practice The FROM instruction initializes a new build stage and sets the Base Image for subsequent instructions. As such, a valid Dockerfile must start with a FROM instruction. ARG is the only instruction that may precede FROM in the Dockerfile. For example: ARG CODE_VERSION=latest FROM base:${CODE_VERSION} Best Practice There can only be one CMD instruction in a Dockerfile. If you list more than one CMD then only the last CMD will take effect. The main purpose of a CMD is to provide defaults for an executing container. Build context is different from Dockerfile The docker build command builds an image from a Dockerfile and a context. The build\u2019s context is the set of files at a specified location PATH or URL. The PATH is a directory on your local filesystem. The URL is a Git repository location. A context is processed recursively. So, a PATH includes any subdirectories and the URL includes the repository and its submodules. This example shows a build command that uses the current directory as context: $ docker build . Sending build context to Docker daemon 6.51 MB ... You use the -f flag with docker build to point to a Dockerfile anywhere in your file system. $ docker build -f /path/to/a/Dockerfile . The build is run by the Docker daemon. The first thing a build process does is send the entire context (recursively) to the daemon. In most cases, it\u2019s best to start with an empty directory as context and keep your Dockerfile in that directory. Add only the files needed for building the Dockerfile. https://docs.docker.com/engine/reference/builder/ To show untagged images, or dangling, use: docker images --filter \"dangling=true\" DTR Backup Process DTR content backed-up \u00b7 Configurations - DTR settings and cluster configurations \u00b7 Repository metadata - Metadata such as image architecture, repositories, images deployed, and size \u00b7 Access control to repos and images - Data about who has access to which images and repositories \u00b7 Notary data - Signatures and digests for images that are signed \u00b7 Scan results - Information about vulnerabilities in your images \u00b7 Certificates and keys - Certificates, public keys, and private keys that are used for mutual TLS communication DTR content NOT backed-up \u00b7 Image content - The images you push to DTR. This can be stored on the file system of the node running DTR, or other storage system, depending on the configuration. Needs to be backed up separately, depends on DTR configuration \u00b7 Users, orgs, teams - Create a UCP backup to back up this data \u00b7 Vulnerability database - Can be redownloaded after a restore Logging Mechanism Docker provides two modes for delivering messages from the container to the log driver: \u00b7 (default) direct, blocking delivery from container to driver \u00b7 non-blocking delivery that stores log messages in an intermediate per-container ring buffer for consumption by driver https://docs.docker.com/config/containers/logging/configure/#configure-the-delivery-mode-of-log-messages-from-container-to-log-driver Start the daemon manually If you don\u2019t want to use a system utility to manage the Docker daemon, or just want to test things out, you can manually run it using the dockerd command. You may need to use sudo, depending on your operating system configuration. When you start Docker this way, it runs in the foreground and sends its logs directly to your terminal. $ dockerd INFO[0000] +job init_networkdriver() INFO[0000] +job serveapi(unix:///var/run/docker.sock) INFO[0000] Listening for HTTP on unix (/var/run/docker.sock) To stop Docker when you have started it manually, issue a Ctrl+C in your terminal In Docker swarm, traffic can be routed to services based on hostname - Swarm Layer7 routing A stack is a collection of services that run on Swarm. Backing up data from one manager node is enough in a UCP. .dockerignore available both in community and enterprise edition. Any 2 processes running on the same host cannot bind to the same port Configuring default address pools\ud83d\udd17 By default Docker Swarm uses a default address pool 10.0.0.0/8 for global scope (overlay) networks. Every network that does not have a subnet specified will have a subnet sequentially allocated from this pool. In some circumstances it may be desirable to use a different default IP address pool for networks. Docker allocates subnet addresses from the address ranges specified by the --default-addr-pool option. For example, a command line option --default-addr-pool 10.10.0.0/16 indicates that Docker will allocate subnets from that /16 address range. If --default-addr-pool-mask-len were unspecified or set explicitly to 24, this would result in 256 /24 networks of the form 10.10.X.0/24 Image signing and Security Scanning are part of DTR If key is compromised, then rotate the key in docker swarm init --advertise-addr <MANAGER-IP> docker swarm join-token [OPTIONS] (worker|manager) docker swarm leave Find port of a container docker [s --filter 'name=web' docker port web docker inspect --format '{{.NetworkSettings.Ports}}' web docker network To manage networks, you can use: docker network COMMAND To display detailed information on one or more networks you can use: docker network inspect [OPTIONS] NETWORK [NETWORK...] https://docs.docker.com/engine/reference/commandline/network_inspect/","title":"Commands"},{"location":"Basics/commands/#find-port-of-a-container","text":"docker [s --filter 'name=web' docker port web docker inspect --format '{{.NetworkSettings.Ports}}' web","title":"Find port of a container"},{"location":"Basics/commands/#docker-network","text":"To manage networks, you can use: docker network COMMAND To display detailed information on one or more networks you can use: docker network inspect [OPTIONS] NETWORK [NETWORK...] https://docs.docker.com/engine/reference/commandline/network_inspect/","title":"docker network"},{"location":"Basics/dockerd/","text":"dockerd By using dockerd you can start the Docker daemon manually. On a typical installation the Docker daemon is started by a system utility, not manually by a user. However, this command can be useful to test things out and for troubleshooting problems. To configure the DNS servers for all Docker containers you have to set the configuration at Docker daemon level. By using dockerd command you can also configure it using flags. For example, to set the DNS server for all Docker containers, use: dockerd --dns IP_ADDRESS To specify multiple DNS servers, use multiple --dns flags. If the container cannot reach any of the IP addresses you specify, Google\u2019s public DNS server 8.8.8.8 is added, so that your container can resolve internet domains. http://dockerlabs.collabnix.com/beginners/components/daemon/ https://docs.docker.com/config/containers/container-networking/ SIGUSR1 If docker daemon is unresponsive, user can send SIGUSR1 signal to docker daemon to force stack trace on daemon to list out all the logs of docker daemon till the point when stack trace was logged in. Stack trace will list out all threads running in docker daemon and helps to find the location of the potential error without terminating daemon process. Concurrent uploads By default the Docker daemon will push five layers of an image at a time. If you are on a low bandwidth connection this may cause timeout issues and you may want to lower this via the --max-concurrent-uploads daemon option. Container inherits default DNS settings from docker daemon Concurrent downloads By default the Docker daemon will pull three layers of an image at a time. If you are on a low bandwidth connection this may cause timeout issues and you may want to lower this via the --max-concurrent-downloads daemon option. See the daemon documentation for more details. windowsfilter On Windows, the Docker daemon supports a single image layer storage driver depending on the image platform: windowsfilter for Windows images, and lcow for Linux containers on Windows. Listen on multiple ports You can configure the Docker daemon to listen to multiple sockets at the same time using multiple -H options: The example below runs the daemon listenin on the default unix socket, and on 2 specific IP addresses on this host: $ sudo dockerd -H unix:///var/run/docker.sock -H tcp://192.168.59.106 -H tcp://10.10.10.2","title":"Dockerd"},{"location":"Basics/dockerd/#dockerd","text":"By using dockerd you can start the Docker daemon manually. On a typical installation the Docker daemon is started by a system utility, not manually by a user. However, this command can be useful to test things out and for troubleshooting problems. To configure the DNS servers for all Docker containers you have to set the configuration at Docker daemon level. By using dockerd command you can also configure it using flags. For example, to set the DNS server for all Docker containers, use: dockerd --dns IP_ADDRESS To specify multiple DNS servers, use multiple --dns flags. If the container cannot reach any of the IP addresses you specify, Google\u2019s public DNS server 8.8.8.8 is added, so that your container can resolve internet domains. http://dockerlabs.collabnix.com/beginners/components/daemon/ https://docs.docker.com/config/containers/container-networking/","title":"dockerd"},{"location":"Basics/dockerd/#sigusr1","text":"If docker daemon is unresponsive, user can send SIGUSR1 signal to docker daemon to force stack trace on daemon to list out all the logs of docker daemon till the point when stack trace was logged in. Stack trace will list out all threads running in docker daemon and helps to find the location of the potential error without terminating daemon process.","title":"SIGUSR1"},{"location":"Basics/dockerd/#concurrent-uploads","text":"By default the Docker daemon will push five layers of an image at a time. If you are on a low bandwidth connection this may cause timeout issues and you may want to lower this via the --max-concurrent-uploads daemon option. Container inherits default DNS settings from docker daemon","title":"Concurrent uploads"},{"location":"Basics/dockerd/#concurrent-downloads","text":"By default the Docker daemon will pull three layers of an image at a time. If you are on a low bandwidth connection this may cause timeout issues and you may want to lower this via the --max-concurrent-downloads daemon option. See the daemon documentation for more details.","title":"Concurrent downloads"},{"location":"Basics/dockerd/#windowsfilter","text":"On Windows, the Docker daemon supports a single image layer storage driver depending on the image platform: windowsfilter for Windows images, and lcow for Linux containers on Windows.","title":"windowsfilter"},{"location":"Basics/dockerd/#listen-on-multiple-ports","text":"You can configure the Docker daemon to listen to multiple sockets at the same time using multiple -H options: The example below runs the daemon listenin on the default unix socket, and on 2 specific IP addresses on this host: $ sudo dockerd -H unix:///var/run/docker.sock -H tcp://192.168.59.106 -H tcp://10.10.10.2","title":"Listen on multiple ports"},{"location":"Basics/install-docker/","text":"Install Docker in Ubuntu sudo apt-get update sudo apt-get -y install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" sudo apt-get install -y docker-ce=5:18.09.5~3-0~ubuntu-bionic docker-ce-cli=5:18.09.5~3-0~ubuntu-bionic containerd.io sudo usermod -a -G docker cloud_user sudo docker run hello-world Fundementals behind adding cloud_user to docker group cat /ectc/passwd cat /etc/group sudo usermod -a -G docker cloud_user /etc/sysconfig/docker # Docker daemon config options Modify the /etc/sysconfig/docker file OPTIONS='--selinux-enabled --log-driver=journald --signature-verification=false -G docker' Restart the docker daemon sudo systemctl daemon-reload sudo systemctl restart docker Now we can run without using sudo docker run hello-world docker run --rm -it --security-opt seccomp=unconfined --cpus=\"0.2\" --memory=\"500m\" docker.io/python import threading def test(): while True: 1000 * 1000 >>> threading.Thread(target=test).start() Install htop to check the processes, CPU and memory consumption yum install htop overall limitations can be provided at /etc/system/,y_limits.slice Description=my slice for docker resources before=slices.target [Slice] CPUAccounting=true CPUQuota=20% MemoryAccounting=true MemoryLimit=200M sudo systemctl daemon-reload sudo systemctl restart docker docker run --rm -it --cgroup-parent=my_limits.slice docker.io/python import threading def test(): while True: 1000 * 1000 >>> threading.Thread(target=test).start()","title":"Install Docker"},{"location":"Basics/install-docker/#install-docker-in-ubuntu","text":"sudo apt-get update sudo apt-get -y install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" sudo apt-get install -y docker-ce=5:18.09.5~3-0~ubuntu-bionic docker-ce-cli=5:18.09.5~3-0~ubuntu-bionic containerd.io sudo usermod -a -G docker cloud_user sudo docker run hello-world Fundementals behind adding cloud_user to docker group cat /ectc/passwd cat /etc/group sudo usermod -a -G docker cloud_user /etc/sysconfig/docker # Docker daemon config options Modify the /etc/sysconfig/docker file OPTIONS='--selinux-enabled --log-driver=journald --signature-verification=false -G docker' Restart the docker daemon sudo systemctl daemon-reload sudo systemctl restart docker Now we can run without using sudo docker run hello-world docker run --rm -it --security-opt seccomp=unconfined --cpus=\"0.2\" --memory=\"500m\" docker.io/python import threading def test(): while True: 1000 * 1000 >>> threading.Thread(target=test).start() Install htop to check the processes, CPU and memory consumption yum install htop","title":"Install Docker in Ubuntu"},{"location":"Basics/install-docker/#overall-limitations-can-be-provided-at-etcsystemy_limitsslice","text":"Description=my slice for docker resources before=slices.target [Slice] CPUAccounting=true CPUQuota=20% MemoryAccounting=true MemoryLimit=200M sudo systemctl daemon-reload sudo systemctl restart docker docker run --rm -it --cgroup-parent=my_limits.slice docker.io/python import threading def test(): while True: 1000 * 1000 >>> threading.Thread(target=test).start()","title":"overall limitations can be provided at /etc/system/,y_limits.slice"},{"location":"Basics/kubernetes/","text":"Kubernetes supports two primary modes of finding a Service, environment variables(injected by the kubelet when pods are created) and DNS. Both are valid options, however, the most common method to discover and reach a service from within Docker Kubernetes Service is to use the embedded DNS service. https://kubernetes.io/docs/concepts/services-networking/service/#discovering-services ClusterIP vs Nodeport vs Ingress Services will receive a cluster-scoped Virtual IP address also known as ClusterIP. ClusterIP is the default method of exposing the service internally. Once the service is created and a VIP is associated with it, every kube-proxy running on every cluster node will program an iptables rule so that all traffic destined to that VIP will be redirected to one of the Service\u2019s backend pods instead. If the service needs to be accessed from outside the cluster, then there are a couple of available options to doing so. Mainly NodePort and LoadBalancer. Ingress is another method that you can use. Ingress is technically not a Type of a service, but it's another method for using Layer 7 based routing to expose your services externally. https://success.docker.com/article/ucp-service-discovery-k8s Calico Docker Universal Control Plane (UCP) uses Calico as the default Kubernetes networking solution. Calico is configured to create a Border Gateway Protocol (BGP) mesh between all nodes in the cluster. PersistentVolume A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. A PV can have a class, which is specified by setting the storageClassName attribute to the name of a StorageClass. A PV of a particular class can only be bound to PVCs requesting that class. A PV with no storageClassName has no class and can only be bound to PVCs that request no particular class. https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes Dynamic volume provisioning Dynamic volume provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to manually make calls to their cloud or storage provider to create new storage volumes, and then create PersistentVolume objects to represent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when it is requested by users. https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/#enabling-dynamic-provisioning #Claims Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster. #PVC to PV - one-to-one mapping A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim. Which concept is responsible for managing internal load balancing for Kubernetes? - ClusterIP Which labels are required to deploy a service with Interlock? Com.docker.interlock.hosts How can we publish services externally in Kubernetes? Nodeport, Ingress controllers Which Kubernetes resources provide an application's resilience? Deployment, Replicaset Which sentences are true about Docker Swarm and Kubernetes networking? Mutual TLS communications ensure control-plane security in Docker Swarm. Kubernetes uses certificates to ensure security for user access and the internal control plane. Role grants - RoleBinding and ClusterRoleBinding Role, subject, resource, namespace - Kubernetes grant commands is used to schedule kubernetes workloads on a node docker node update \\ --label-add com.docker.ucp.orchestrator.kubernetes=true","title":"Kubernetes"},{"location":"Basics/kubernetes/#clusterip-vs-nodeport-vs-ingress","text":"Services will receive a cluster-scoped Virtual IP address also known as ClusterIP. ClusterIP is the default method of exposing the service internally. Once the service is created and a VIP is associated with it, every kube-proxy running on every cluster node will program an iptables rule so that all traffic destined to that VIP will be redirected to one of the Service\u2019s backend pods instead. If the service needs to be accessed from outside the cluster, then there are a couple of available options to doing so. Mainly NodePort and LoadBalancer. Ingress is another method that you can use. Ingress is technically not a Type of a service, but it's another method for using Layer 7 based routing to expose your services externally. https://success.docker.com/article/ucp-service-discovery-k8s Calico Docker Universal Control Plane (UCP) uses Calico as the default Kubernetes networking solution. Calico is configured to create a Border Gateway Protocol (BGP) mesh between all nodes in the cluster.","title":"ClusterIP vs Nodeport vs Ingress"},{"location":"Basics/kubernetes/#persistentvolume","text":"A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. A PV can have a class, which is specified by setting the storageClassName attribute to the name of a StorageClass. A PV of a particular class can only be bound to PVCs requesting that class. A PV with no storageClassName has no class and can only be bound to PVCs that request no particular class. https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes","title":"PersistentVolume"},{"location":"Basics/kubernetes/#dynamic-volume-provisioning","text":"Dynamic volume provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to manually make calls to their cloud or storage provider to create new storage volumes, and then create PersistentVolume objects to represent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when it is requested by users. https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/#enabling-dynamic-provisioning","title":"Dynamic volume provisioning"},{"location":"Basics/kubernetes/#claims","text":"Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster.","title":"#Claims"},{"location":"Basics/kubernetes/#pvc-to-pv-one-to-one-mapping","text":"A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.","title":"#PVC to PV - one-to-one mapping"},{"location":"Basics/kubernetes/#which-concept-is-responsible-for-managing-internal-load-balancing-for-kubernetes-clusterip","text":"","title":"Which concept is responsible for managing internal load balancing for Kubernetes? - ClusterIP"},{"location":"Basics/kubernetes/#which-labels-are-required-to-deploy-a-service-with-interlock-comdockerinterlockhosts","text":"","title":"Which labels are required to deploy a service with Interlock? Com.docker.interlock.hosts"},{"location":"Basics/kubernetes/#how-can-we-publish-services-externally-in-kubernetes-nodeport-ingress-controllers","text":"","title":"How can we publish services externally in Kubernetes? Nodeport, Ingress controllers"},{"location":"Basics/kubernetes/#which-kubernetes-resources-provide-an-applications-resilience-deployment-replicaset","text":"","title":"Which Kubernetes resources provide an application's resilience? Deployment, Replicaset"},{"location":"Basics/kubernetes/#which-sentences-are-true-about-docker-swarm-and-kubernetes-networking","text":"Mutual TLS communications ensure control-plane security in Docker Swarm. Kubernetes uses certificates to ensure security for user access and the internal control plane. Role grants - RoleBinding and ClusterRoleBinding Role, subject, resource, namespace - Kubernetes grant","title":"Which sentences are true about Docker Swarm and Kubernetes networking?"},{"location":"Basics/kubernetes/#commands-is-used-to-schedule-kubernetes-workloads-on-a-node","text":"docker node update \\ --label-add com.docker.ucp.orchestrator.kubernetes=true","title":"commands is used to schedule kubernetes workloads on a node"},{"location":"Basics/probes/","text":"When should you use a liveness probe? If the process in your container is able to crash on its own whenever it encounters an issue or becomes unhealthy, you do not necessarily need a liveness probe; the kubelet will automatically perform the correct action in accordance with the Pod's restartPolicy. If you'd like your container to be killed and restarted if a probe fails, then specify a liveness probe, and specify a restartPolicy of Always or OnFailure. When should you use a readiness probe? If you'd like to start sending traffic to a Pod only when a probe succeeds, specify a readiness probe. In this case, the readiness probe might be the same as the liveness probe, but the existence of the readiness probe in the spec means that the Pod will start without receiving any traffic and only start receiving traffic after the probe starts succeeding. If you want your container to be able to take itself down for maintenance, you can specify a readiness probe that checks an endpoint specific to readiness that is different from the liveness probe. If your app has a strict dependency on back-end services, you can implement both a liveness and a readiness probe. The liveness probe passes when the app itself is healthy, but the readiness probe additionally checks that each required back-end service is available. This helps you avoid directing traffic to Pods that can only respond with error messages. If your container needs to work on loading large data, configuration files, or migrations during startup, you can use a startup probe. However, if you want to detect the difference between an app that has failed and an app that is still processing its startup data, you might prefer a readiness probe. Note: If you want to be able to drain requests when the Pod is deleted, you do not necessarily need a readiness probe; on deletion, the Pod automatically puts itself into an unready state regardless of whether the readiness probe exists. The Pod remains in the unready state while it waits for the containers in the Pod to stop. When should you use a startup probe? Startup probes are useful for Pods that have containers that take a long time to come into service. Rather than set a long liveness interval, you can configure a separate configuration for probing the container as it starts up, allowing a time longer than the liveness interval would allow. If your container usually starts in more than initialDelaySeconds + failureThreshold \u00d7 periodSeconds, you should specify a startup probe that checks the same endpoint as the liveness probe. The default for periodSeconds is 10s. You should then set its failureThreshold high enough to allow the container to start, without changing the default values of the liveness probe. This helps to protect against deadlocks.","title":"Probes"},{"location":"Basics/probes/#when-should-you-use-a-liveness-probe","text":"If the process in your container is able to crash on its own whenever it encounters an issue or becomes unhealthy, you do not necessarily need a liveness probe; the kubelet will automatically perform the correct action in accordance with the Pod's restartPolicy. If you'd like your container to be killed and restarted if a probe fails, then specify a liveness probe, and specify a restartPolicy of Always or OnFailure.","title":"When should you use a liveness probe?"},{"location":"Basics/probes/#when-should-you-use-a-readiness-probe","text":"If you'd like to start sending traffic to a Pod only when a probe succeeds, specify a readiness probe. In this case, the readiness probe might be the same as the liveness probe, but the existence of the readiness probe in the spec means that the Pod will start without receiving any traffic and only start receiving traffic after the probe starts succeeding. If you want your container to be able to take itself down for maintenance, you can specify a readiness probe that checks an endpoint specific to readiness that is different from the liveness probe. If your app has a strict dependency on back-end services, you can implement both a liveness and a readiness probe. The liveness probe passes when the app itself is healthy, but the readiness probe additionally checks that each required back-end service is available. This helps you avoid directing traffic to Pods that can only respond with error messages. If your container needs to work on loading large data, configuration files, or migrations during startup, you can use a startup probe. However, if you want to detect the difference between an app that has failed and an app that is still processing its startup data, you might prefer a readiness probe. Note: If you want to be able to drain requests when the Pod is deleted, you do not necessarily need a readiness probe; on deletion, the Pod automatically puts itself into an unready state regardless of whether the readiness probe exists. The Pod remains in the unready state while it waits for the containers in the Pod to stop.","title":"When should you use a readiness probe?"},{"location":"Basics/probes/#when-should-you-use-a-startup-probe","text":"Startup probes are useful for Pods that have containers that take a long time to come into service. Rather than set a long liveness interval, you can configure a separate configuration for probing the container as it starts up, allowing a time longer than the liveness interval would allow. If your container usually starts in more than initialDelaySeconds + failureThreshold \u00d7 periodSeconds, you should specify a startup probe that checks the same endpoint as the liveness probe. The default for periodSeconds is 10s. You should then set its failureThreshold high enough to allow the container to start, without changing the default values of the liveness probe. This helps to protect against deadlocks.","title":"When should you use a startup probe?"},{"location":"DockerEE/dct/","text":"Docker Content trust When communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and the publisher of all the data a system operates on. You use the Docker Engine to push and pull images (data) to a public or private registry. Content trust gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel. Docker Content Trust (DCT) provides the ability to use digital signatures for data sent to and received from remote Docker registries. These signatures allow client-side or runtime verification of the integrity and publisher of specific image tags. Docker Content Trust (DCT) provides the ability to use digital signatures for data sent to and received from remote Docker registries. These signatures allow client-side or runtime verification of the integrity and publisher of specific image tags. https://docs.docker.com/engine/security/trust/content_trust/#about-docker-content-trust-dct Docker Content Trust Signature Verification The Docker Engine can be configured to only run signed images. The Docker Content Trust signature verification feature is built directly into the dockerd binary. This is configured in the Dockerd configuration file. To enable this feature, trustpinning can be configured in daemon.json, whereby only repositories signed with a user-specified root key can be pulled and run. Which of these keys requires a passphrase to unlock it while signing images? a) Target b) Snapshot How can we ensure that a specific image is deployed in production? a) By using the image's hash for deploying containers. b) Signing images will ensure their tagging and provenance. Docker Content Trust Keys Trust for an image tag is managed through the use of signing keys. A key set is created when an operation using DCT is first invoked. A key set consists of the following classes of keys: an offline key that is the root of DCT for an image tag repository or tagging keys that sign tags server-managed keys such as the timestamp key, which provides freshness security guarantees for your repository Delegations for content trust Delegations in Docker Content Trust (DCT) allow you to control who can and cannot sign an image tag. A delegation will have a pair of private and public delegation keys. A delegation could contain multiple pairs of keys and contributors in order to a) allow multiple users to be part of a delegation, and b) to support key rotation.","title":"Docker Content Trust"},{"location":"DockerEE/dct/#docker-content-trust","text":"When communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and the publisher of all the data a system operates on. You use the Docker Engine to push and pull images (data) to a public or private registry. Content trust gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel. Docker Content Trust (DCT) provides the ability to use digital signatures for data sent to and received from remote Docker registries. These signatures allow client-side or runtime verification of the integrity and publisher of specific image tags. Docker Content Trust (DCT) provides the ability to use digital signatures for data sent to and received from remote Docker registries. These signatures allow client-side or runtime verification of the integrity and publisher of specific image tags. https://docs.docker.com/engine/security/trust/content_trust/#about-docker-content-trust-dct Docker Content Trust Signature Verification The Docker Engine can be configured to only run signed images. The Docker Content Trust signature verification feature is built directly into the dockerd binary. This is configured in the Dockerd configuration file. To enable this feature, trustpinning can be configured in daemon.json, whereby only repositories signed with a user-specified root key can be pulled and run.","title":"Docker Content trust"},{"location":"DockerEE/dct/#which-of-these-keys-requires-a-passphrase-to-unlock-it-while-signing-images","text":"a) Target b) Snapshot","title":"Which of these keys requires a passphrase to unlock it while signing images?"},{"location":"DockerEE/dct/#how-can-we-ensure-that-a-specific-image-is-deployed-in-production","text":"a) By using the image's hash for deploying containers. b) Signing images will ensure their tagging and provenance.","title":"How can we ensure that a specific image is deployed in production?"},{"location":"DockerEE/dct/#docker-content-trust-keys","text":"Trust for an image tag is managed through the use of signing keys. A key set is created when an operation using DCT is first invoked. A key set consists of the following classes of keys: an offline key that is the root of DCT for an image tag repository or tagging keys that sign tags server-managed keys such as the timestamp key, which provides freshness security guarantees for your repository","title":"Docker Content Trust Keys"},{"location":"DockerEE/dct/#delegations-for-content-trust","text":"Delegations in Docker Content Trust (DCT) allow you to control who can and cannot sign an image tag. A delegation will have a pair of private and public delegation keys. A delegation could contain multiple pairs of keys and contributors in order to a) allow multiple users to be part of a delegation, and b) to support key rotation.","title":"Delegations for content trust"},{"location":"DockerEE/docker-hub/","text":"Docker Hub Docker Hub is a service provided by Docker for finding and sharing container images with your team. It provides the following major features: Repositories: Push and pull container images. Teams & Organizations: Manage access to private repositories of container images. Official Images: Pull and use high-quality container images provided by Docker. Publisher Images: Pull and use high-quality container images provided by external vendors. Certified images also include support and guarantee compatibility with Docker Enterprise. Builds: Automatically build container images from GitHub and Bitbucket and push them to Docker Hub. Webhooks: Trigger actions after a successful push to a repository to integrate Docker Hub with other services.","title":"Docker Hub"},{"location":"DockerEE/docker-hub/#docker-hub","text":"Docker Hub is a service provided by Docker for finding and sharing container images with your team. It provides the following major features: Repositories: Push and pull container images. Teams & Organizations: Manage access to private repositories of container images. Official Images: Pull and use high-quality container images provided by Docker. Publisher Images: Pull and use high-quality container images provided by external vendors. Certified images also include support and guarantee compatibility with Docker Enterprise. Builds: Automatically build container images from GitHub and Bitbucket and push them to Docker Hub. Webhooks: Trigger actions after a successful push to a repository to integrate Docker Hub with other services.","title":"Docker Hub"},{"location":"DockerEE/dtr/","text":"Docker Trusted Registry (DTR) Docker Trusted Registry can scan images in your repositories to verify that they are free from known security vulnerabilities or exposures, using Docker Security Scanning. Docker Security Scanning is available as an add-on to Docker Trusted Registry, and an administrator configures it for your DTR instance. Image Immutability As of DTR 2.3.0, there is an option to set a repository to Immutable. Setting a repository to Immutable means the tags can not be overwritten. This is a great feature for ensure the base images do not change over time. This next example is of the Alpine base image. Ideally CI would update the base image and push to DTR with a specific tag. Being Immutable simply guarantees that an authorized user can always go back to the specific tag and trust it has not changed. An Image Promotion Policy can extend on this. Insecure registries While it\u2019s highly recommended to secure your registry using a TLS certificate issued by a known CA, you can choose to use self-signed certificates, or use your registry over an unencrypted HTTP connection. Either of these choices involves security trade-offs and additional configuration steps. This procedure configures Docker to entirely disregard security for your registry. Edit the daemon.json file, whose default location is /etc/docker/daemon.json. Add the following content. { \"insecure-registries\": [\"myregistrydomain.com:5000\"] } Restart Docker for the changes to take effect. Repeat these steps on every Engine host that wants to access your registry. Alternatively, you can pass the --engine-insecure-registry flag when the docker engine is started. Notary As of Docker Engine 18.06 there is a docker trust command that will streamline the image signing process. The old is Notary. Notary is a tool for publishing and managing trusted collections of content. Publishers can digitally sign collections and consumers can verify integrity and origin of content. This ability is built on a straightforward key management and signing interface to create signed collections and configure trusted publishers. Vulnerabilities From the Components view, the CVE number, a link to CVE database, file path, layers affected, severity, and description of severity are available. scan Only users with write access to a repository can manually start a scan. Users with read-only access can view the scan results, but cannot start a new scan. Image scan is a feature of Docker trusted Repository DTR, which is part of Docker Entreprise Edition. To use the image scanning feature, make sure that you or your organization has purchased a DTR license that includes Docker Security Scanning, and that your Docker ID can access and download this license from the Docker Hub. After you get the license, you\u2019ll need to Enable DTR security scanning. Docker Trusted Registry can scan images in your repositories to verify that they are free from known security vulnerabilities or exposures, using Docker Security Scanning. Docker Security Scanning is available as an add-on to Docker Trusted Registry, and an administrator configures it for your DTR instance Garbage Collection Garbage collection is an often-overlooked area from a security standpoint. Old, out-of-date images may contain security flaws or exploitable vulnerabilities; removing unnecessary images is important. Garbage collection is a feature that ensures that unreferenced images (and layers) are removed. In the context of the Docker registry, garbage collection is the process of removing blobs from the filesystem when they are no longer referenced by a manifest. Blobs can include both layers and manifests. Registry data can occupy considerable amounts of disk space. In addition, garbage collection can be a security consideration, when it is desirable to ensure that certain layers no longer exist on the filesystem. Install DTR You can install DTR on-premises or on a cloud provider. To install DTR, all nodes must: Be a worker node managed by UCP (Universal Control Plane). DTR replicas are installed one on each UCP worker node. Have a fixed hostname. Networks used by DTR To allow containers to communicate, when installing DTR the overlay networks should be created. dtr-ol this allows DTR components running on different nodes to communicate, to replicate DTR data Health checks DTR also exposes several endpoints you can use to assess if a DTR replica is healthy or not: /health : Checks if the several components of a DTR replica are healthy, and returns a simple json response. This is useful for load balancing or other automated health check tasks. /load_balancer_status : Checks if the several components of a DTR replica can be reached, and displays that information in a table. This is useful for an administrator to gauge the status of a DTR replica. /nginx_status : Returns the number of connections being handled by the NGINX front-end used by DTR. /api/v0/meta/cluster_status : Returns extensive information about all DTR replicas. DTR content backed-up Configurations - DTR settings and cluster configurations Repository metadata - Metadata such as image architecture, repositories, images deployed, and size Access control to repos and images - Data about who has access to which images and repositories Notary data - Signatures and digests for images that are signed Scan results - Information about vulnerabilities in your images Certificates and keys - Certificates, public keys, and private keys that are used for mutual TLS communication DTR content NOT backed-up Image content - The images you push to DTR. This can be stored on the file system of the node running DTR, or other storage system, depending on the configuration. Needs to be backed up separately, depends on DTR configuration Users, orgs, teams - Create a UCP backup to back up this data Vulnerability database - Can be redownloaded after a restore HA For high-availability you can deploy multiple DTR replicas, one on each UCP worker node. Cache deployment strategy The main reason to use a DTR cache is so that users can pull images from a service that\u2019s geographically closer to them. In this example, a company has developers spread across three locations: United States, Asia, and Europe. Developers working in the US office can pull their images from DTR without problem, but developers in the Asia and Europe offices complain that it takes them a long time to pulls images. To address that, you can deploy DTR caches in the Asia and Europe offices, so that developers working from there can pull images much faster. What is required to deploy DTR? a) A Docker Enterprise license from Docker Hub and an appropriate repository URL b) Docker Enterprise Engine and Docker UCP Which endpoints are provided to verify DTR and UCP nodes' health? a) DTR provides /_ping, /nginx_status, and /api/v0/meta/cluster_status. c) DTR and UCP provide /_ping. Which internal networks are deployed for DTR? dtr-ol only write access can scan Only users with write access to a repository can manually start a scan. Users with read-only access can view the scan results, but cannot start a new scan. The repository name needs to be unique in that namespace, can be two to 255 characters, and can only contain lowercase letters, numbers, hyphens (-), and underscores (_). notary {cmd} docker.io/library/ correct GUN format for naming Docker Images for Notary Client tag name A tag name must be valid ASCII and may contain lowercase and uppercase letters, digits, underscores, periods and dashes. A tag name may not start with a period or a dash and may contain a maximum of 128 characters. install on a worker node DTR needs to be installed on a worker node that is being managed by UCP. Set a tag limit In addition to pruning policies, you can also set tag limits on repositories that you manage to restrict the number of tags on a given repository. Repository tag limits are processed in a first in first out (FIFO) manner. For example, if you set a tag limit of 2, adding a third tag would push out the first.","title":"Docker Trust Registry"},{"location":"DockerEE/dtr/#docker-trusted-registry-dtr","text":"Docker Trusted Registry can scan images in your repositories to verify that they are free from known security vulnerabilities or exposures, using Docker Security Scanning. Docker Security Scanning is available as an add-on to Docker Trusted Registry, and an administrator configures it for your DTR instance.","title":"Docker Trusted Registry (DTR)"},{"location":"DockerEE/dtr/#image-immutability","text":"As of DTR 2.3.0, there is an option to set a repository to Immutable. Setting a repository to Immutable means the tags can not be overwritten. This is a great feature for ensure the base images do not change over time. This next example is of the Alpine base image. Ideally CI would update the base image and push to DTR with a specific tag. Being Immutable simply guarantees that an authorized user can always go back to the specific tag and trust it has not changed. An Image Promotion Policy can extend on this.","title":"Image Immutability"},{"location":"DockerEE/dtr/#insecure-registries","text":"While it\u2019s highly recommended to secure your registry using a TLS certificate issued by a known CA, you can choose to use self-signed certificates, or use your registry over an unencrypted HTTP connection. Either of these choices involves security trade-offs and additional configuration steps. This procedure configures Docker to entirely disregard security for your registry. Edit the daemon.json file, whose default location is /etc/docker/daemon.json. Add the following content. { \"insecure-registries\": [\"myregistrydomain.com:5000\"] } Restart Docker for the changes to take effect. Repeat these steps on every Engine host that wants to access your registry. Alternatively, you can pass the --engine-insecure-registry flag when the docker engine is started.","title":"Insecure registries"},{"location":"DockerEE/dtr/#notary","text":"As of Docker Engine 18.06 there is a docker trust command that will streamline the image signing process. The old is Notary. Notary is a tool for publishing and managing trusted collections of content. Publishers can digitally sign collections and consumers can verify integrity and origin of content. This ability is built on a straightforward key management and signing interface to create signed collections and configure trusted publishers.","title":"Notary"},{"location":"DockerEE/dtr/#vulnerabilities","text":"From the Components view, the CVE number, a link to CVE database, file path, layers affected, severity, and description of severity are available.","title":"Vulnerabilities"},{"location":"DockerEE/dtr/#scan","text":"Only users with write access to a repository can manually start a scan. Users with read-only access can view the scan results, but cannot start a new scan. Image scan is a feature of Docker trusted Repository DTR, which is part of Docker Entreprise Edition. To use the image scanning feature, make sure that you or your organization has purchased a DTR license that includes Docker Security Scanning, and that your Docker ID can access and download this license from the Docker Hub. After you get the license, you\u2019ll need to Enable DTR security scanning. Docker Trusted Registry can scan images in your repositories to verify that they are free from known security vulnerabilities or exposures, using Docker Security Scanning. Docker Security Scanning is available as an add-on to Docker Trusted Registry, and an administrator configures it for your DTR instance","title":"scan"},{"location":"DockerEE/dtr/#garbage-collection","text":"Garbage collection is an often-overlooked area from a security standpoint. Old, out-of-date images may contain security flaws or exploitable vulnerabilities; removing unnecessary images is important. Garbage collection is a feature that ensures that unreferenced images (and layers) are removed. In the context of the Docker registry, garbage collection is the process of removing blobs from the filesystem when they are no longer referenced by a manifest. Blobs can include both layers and manifests. Registry data can occupy considerable amounts of disk space. In addition, garbage collection can be a security consideration, when it is desirable to ensure that certain layers no longer exist on the filesystem.","title":"Garbage Collection"},{"location":"DockerEE/dtr/#install-dtr","text":"You can install DTR on-premises or on a cloud provider. To install DTR, all nodes must: Be a worker node managed by UCP (Universal Control Plane). DTR replicas are installed one on each UCP worker node. Have a fixed hostname.","title":"Install DTR"},{"location":"DockerEE/dtr/#networks-used-by-dtr","text":"To allow containers to communicate, when installing DTR the overlay networks should be created. dtr-ol this allows DTR components running on different nodes to communicate, to replicate DTR data","title":"Networks used by DTR"},{"location":"DockerEE/dtr/#health-checks","text":"DTR also exposes several endpoints you can use to assess if a DTR replica is healthy or not: /health : Checks if the several components of a DTR replica are healthy, and returns a simple json response. This is useful for load balancing or other automated health check tasks. /load_balancer_status : Checks if the several components of a DTR replica can be reached, and displays that information in a table. This is useful for an administrator to gauge the status of a DTR replica. /nginx_status : Returns the number of connections being handled by the NGINX front-end used by DTR. /api/v0/meta/cluster_status : Returns extensive information about all DTR replicas.","title":"Health checks"},{"location":"DockerEE/dtr/#dtr-content-backed-up","text":"Configurations - DTR settings and cluster configurations Repository metadata - Metadata such as image architecture, repositories, images deployed, and size Access control to repos and images - Data about who has access to which images and repositories Notary data - Signatures and digests for images that are signed Scan results - Information about vulnerabilities in your images Certificates and keys - Certificates, public keys, and private keys that are used for mutual TLS communication","title":"DTR content backed-up"},{"location":"DockerEE/dtr/#dtr-content-not-backed-up","text":"Image content - The images you push to DTR. This can be stored on the file system of the node running DTR, or other storage system, depending on the configuration. Needs to be backed up separately, depends on DTR configuration Users, orgs, teams - Create a UCP backup to back up this data Vulnerability database - Can be redownloaded after a restore","title":"DTR content NOT backed-up"},{"location":"DockerEE/dtr/#ha","text":"For high-availability you can deploy multiple DTR replicas, one on each UCP worker node.","title":"HA"},{"location":"DockerEE/dtr/#cache-deployment-strategy","text":"The main reason to use a DTR cache is so that users can pull images from a service that\u2019s geographically closer to them. In this example, a company has developers spread across three locations: United States, Asia, and Europe. Developers working in the US office can pull their images from DTR without problem, but developers in the Asia and Europe offices complain that it takes them a long time to pulls images. To address that, you can deploy DTR caches in the Asia and Europe offices, so that developers working from there can pull images much faster.","title":"Cache deployment strategy"},{"location":"DockerEE/dtr/#what-is-required-to-deploy-dtr","text":"a) A Docker Enterprise license from Docker Hub and an appropriate repository URL b) Docker Enterprise Engine and Docker UCP","title":"What is required to deploy DTR?"},{"location":"DockerEE/dtr/#which-endpoints-are-provided-to-verify-dtr-and-ucp-nodes-health","text":"a) DTR provides /_ping, /nginx_status, and /api/v0/meta/cluster_status. c) DTR and UCP provide /_ping.","title":"Which endpoints are provided to verify DTR and UCP nodes' health?"},{"location":"DockerEE/dtr/#which-internal-networks-are-deployed-for-dtr-dtr-ol","text":"","title":"Which internal networks are deployed for DTR? dtr-ol"},{"location":"DockerEE/dtr/#only-write-access-can-scan","text":"Only users with write access to a repository can manually start a scan. Users with read-only access can view the scan results, but cannot start a new scan. The repository name needs to be unique in that namespace, can be two to 255 characters, and can only contain lowercase letters, numbers, hyphens (-), and underscores (_).","title":"only write access can scan"},{"location":"DockerEE/dtr/#notary-cmd-dockeriolibrary","text":"correct GUN format for naming Docker Images for Notary Client","title":"notary {cmd} docker.io/library/"},{"location":"DockerEE/dtr/#tag-name","text":"A tag name must be valid ASCII and may contain lowercase and uppercase letters, digits, underscores, periods and dashes. A tag name may not start with a period or a dash and may contain a maximum of 128 characters.","title":"tag name"},{"location":"DockerEE/dtr/#install-on-a-worker-node","text":"DTR needs to be installed on a worker node that is being managed by UCP.","title":"install on a worker node"},{"location":"DockerEE/dtr/#set-a-tag-limit","text":"In addition to pruning policies, you can also set tag limits on repositories that you manage to restrict the number of tags on a given repository. Repository tag limits are processed in a first in first out (FIFO) manner. For example, if you set a tag limit of 2, adding a third tag would push out the first.","title":"Set a tag limit"},{"location":"DockerEE/ee/","text":"Limit Root Access to Node Docker Enterprise uses a completely separate authentication backend from the host, providing a clear separation of duties. Docker Enterprise can leverage an existing LDAP/AD infrastructure for authentication. It even utilizes RBAC Labels to control access to objects like images and running containers, meaning teams of users can be given full access to running containers. With this access, users can watch the logs and execute a shell inside the running container without needing to ever log into the host. Limiting the number of users that have access to the host reduces the attack surface. --cap-drop, --cap-add Linux capabilities are an even more granular way of reducing surface area. Docker Engine has a default list of capabilities that are kept for newly-created containers, and by using the --cap-drop option for docker run, users can exclude additional capabilities from being used by processes inside the container on a capability-by-capability basis. All privileges can be dropped with the --user option. Likewise, capabilities that are, by default, not granted to new containers can be added with the --cap-add option. This is discouraged unless absolutely necessary, and using --cap-add=ALL is highly discouraged. Limiting the capabilities of a container reduces the attack surface. Installation of docker engine You can install Docker Engine in different ways, depending on your needs: Most users set up Docker\u2019s repositories and install from them, for ease of installation and upgrade tasks. This is the recommended approach. Some users download the RPM package and install it manually and manage upgrades completely manually. This is useful in situations such as installing Docker on air-gapped systems with no access to the internet. In testing and development environments, some users choose to use automated convenience scripts to install Docker. Docker Enterprise components Docker Enterprise has three major components, which together enable a full software supply chain, from image creation, to secure image storage, to secure image deployment. Docker Engine - Enterprise: The commercially supported Docker engine for creating images and running them in Docker containers. Docker Trusted Registry (DTR): The production-grade image storage solution from Docker. DTR is designed to scale horizontally as your usage increases. You can add more replicas to make DTR scale to your demand and for high availability. All DTR replicas run the same set of services, and changes to their configuration are propagated automatically to other replicas. Universal Control Plane (UCP): Deploys applications from images, by managing orchestrators, like Kubernetes and Swarm. UCP is designed for high availability (HA). You can join multiple UCP manager nodes to the cluster, and if one manager node fails, another takes its place automatically without impact to the cluster. Changes to the configuration of one UCP manager node are propagated automatically to other nodes. Container format Docker Engine combines the namespaces, control groups, and UnionFS into a wrapper called a container format. The default container format is libcontainer. Backup To back up Docker Enterprise, you must create individual backups for each of the following components: Back up Docker Swarm. Back up Swarm resources like service and network definitions. Back up Universal Control Plane (UCP). Back up UCP configurations such as Access control, Certificates and keys, volumes = All UCP named volumes, which include all UCP component certs and data, Monitoring data gathered by UCP, Back up Docker Trusted Registry (DTR). Back up DTR configurations, images, and metadata, Repository metadata, Access control to repos and images, Notary data, Scan results. If you do not create backups for all components, you cannot restore your deployment to its previous state. Role-Based Access Control (RBAC) is part of UCP. Bundle A user's Docker bundle includes all the environment files and certificates required for using the CaaS platform. Team\u2019s permissions Permissions are cumulative. For example, if you have Write permissions, you automatically have Read permissions: Read access allows users to view, search, and pull a private repository in the same way as they can a public repository. Write access allows users to push to repositories on Docker Hub. Admin access allows users to modify the repositories \u201cDescription\u201d, \u201cCollaborators\u201d rights, \u201cPublic/Private\u201d visibility, and \u201cDelete\u201d. webhooks You can use webhooks to cause an action in another service in response to a push event in the repository. Webhooks are POST requests sent to a URL you define in Docker Hub. orchestrator type of a newly promoted worker node - Mixed","title":"Docker Enterprise Edition"},{"location":"DockerEE/ee/#limit-root-access-to-node","text":"Docker Enterprise uses a completely separate authentication backend from the host, providing a clear separation of duties. Docker Enterprise can leverage an existing LDAP/AD infrastructure for authentication. It even utilizes RBAC Labels to control access to objects like images and running containers, meaning teams of users can be given full access to running containers. With this access, users can watch the logs and execute a shell inside the running container without needing to ever log into the host. Limiting the number of users that have access to the host reduces the attack surface.","title":"Limit Root Access to Node"},{"location":"DockerEE/ee/#-cap-drop-cap-add","text":"Linux capabilities are an even more granular way of reducing surface area. Docker Engine has a default list of capabilities that are kept for newly-created containers, and by using the --cap-drop option for docker run, users can exclude additional capabilities from being used by processes inside the container on a capability-by-capability basis. All privileges can be dropped with the --user option. Likewise, capabilities that are, by default, not granted to new containers can be added with the --cap-add option. This is discouraged unless absolutely necessary, and using --cap-add=ALL is highly discouraged. Limiting the capabilities of a container reduces the attack surface.","title":"--cap-drop, --cap-add"},{"location":"DockerEE/ee/#installation-of-docker-engine","text":"You can install Docker Engine in different ways, depending on your needs: Most users set up Docker\u2019s repositories and install from them, for ease of installation and upgrade tasks. This is the recommended approach. Some users download the RPM package and install it manually and manage upgrades completely manually. This is useful in situations such as installing Docker on air-gapped systems with no access to the internet. In testing and development environments, some users choose to use automated convenience scripts to install Docker.","title":"Installation of docker engine"},{"location":"DockerEE/ee/#docker-enterprise-components","text":"Docker Enterprise has three major components, which together enable a full software supply chain, from image creation, to secure image storage, to secure image deployment. Docker Engine - Enterprise: The commercially supported Docker engine for creating images and running them in Docker containers. Docker Trusted Registry (DTR): The production-grade image storage solution from Docker. DTR is designed to scale horizontally as your usage increases. You can add more replicas to make DTR scale to your demand and for high availability. All DTR replicas run the same set of services, and changes to their configuration are propagated automatically to other replicas. Universal Control Plane (UCP): Deploys applications from images, by managing orchestrators, like Kubernetes and Swarm. UCP is designed for high availability (HA). You can join multiple UCP manager nodes to the cluster, and if one manager node fails, another takes its place automatically without impact to the cluster. Changes to the configuration of one UCP manager node are propagated automatically to other nodes.","title":"Docker Enterprise components"},{"location":"DockerEE/ee/#container-format","text":"Docker Engine combines the namespaces, control groups, and UnionFS into a wrapper called a container format. The default container format is libcontainer.","title":"Container format"},{"location":"DockerEE/ee/#backup","text":"To back up Docker Enterprise, you must create individual backups for each of the following components: Back up Docker Swarm. Back up Swarm resources like service and network definitions. Back up Universal Control Plane (UCP). Back up UCP configurations such as Access control, Certificates and keys, volumes = All UCP named volumes, which include all UCP component certs and data, Monitoring data gathered by UCP, Back up Docker Trusted Registry (DTR). Back up DTR configurations, images, and metadata, Repository metadata, Access control to repos and images, Notary data, Scan results. If you do not create backups for all components, you cannot restore your deployment to its previous state. Role-Based Access Control (RBAC) is part of UCP.","title":"Backup"},{"location":"DockerEE/ee/#bundle","text":"A user's Docker bundle includes all the environment files and certificates required for using the CaaS platform.","title":"Bundle"},{"location":"DockerEE/ee/#teams-permissions","text":"Permissions are cumulative. For example, if you have Write permissions, you automatically have Read permissions: Read access allows users to view, search, and pull a private repository in the same way as they can a public repository. Write access allows users to push to repositories on Docker Hub. Admin access allows users to modify the repositories \u201cDescription\u201d, \u201cCollaborators\u201d rights, \u201cPublic/Private\u201d visibility, and \u201cDelete\u201d.","title":"Team\u2019s permissions"},{"location":"DockerEE/ee/#webhooks","text":"You can use webhooks to cause an action in another service in response to a push event in the repository. Webhooks are POST requests sent to a URL you define in Docker Hub. orchestrator type of a newly promoted worker node - Mixed","title":"webhooks"},{"location":"DockerEE/ucp/","text":"UCP UCP always runs with HTTPS enabled. When you connect to UCP, you need to make sure that the hostname that you use to connect is recognized by UCP\u2019s certificates. If, for instance, you put UCP behind a load balancer that forwards its traffic to your UCP instance, your requests will be for the load balancer\u2019s hostname or IP address, not UCP\u2019s. UCP will reject these requests unless you include the load balancer\u2019s address as a Subject Alternative Name (or SAN) in UCP\u2019s certificates. If you use your own TLS certificates, make sure that they have the correct SAN values If you want to use the self-signed certificate that UCP has out of the box, you can set up the SANs when you install UCP with the --san argument. You can also add them after installation. Sign images using notary - To sign images in a way that UCP trusts them you need to: - Configure your Notary client - Initialize trust metadata for the repository - Delegate signing to the keys in your UCP client bundle Collections Docker Enterprise enables controlling access to swarm resources by using collections. A collection is a grouping of swarm cluster resources that you access by specifying a directory-like path. Before grants can be implemented, collections need to be designed to group resources in a way that makes sense for an organization. The following example shows the potential access policy of an organization. Consider an organization with two application teams, Mobile and Payments, that share cluster hardware resources, but still need to segregate access to the applications. Collections should be designed to map to the organizational structure desired, in this case the two application teams. Collections are implemented in UCP through the use of Docker labels. All resources within a given collection are labeled with the collection, /production/mobile for instance. Collections are groupings of objects within UCP. A collection can be made up of one or many of nodes, stacks, containers, services, volumes, networks, secrets, or configs \u2014 or it can hold other collections. To associate a node or a stack or any resource with a collection, that resource should share the label com.docker.ucp.access.label with the collection. A resource can be associated with zero or multiple collections, and a collection can have zero or multiple resources or other child collections in it. Collections within collections allow the structuring of resource objects in a hierarchical nature and can significantly simplify access control. Access provided at a top level collection is inherited by all its children, including any child collections. LDAP Docker UCP integrates with LDAP directory services, so that you can manage users and groups from your organization\u2019s directory and it will automatically propagate that information to UCP and DTR. UCP services include a component called ucp-auth-api, which is the centralized service for identity and authentication used by UCP and DTR. All UCP services are exposed using HTTPS, to ensure all communications between clients and UCP are encrypted. By default, this is done using self-signed TLS certificates that are not trusted by client tools like web browsers. So when you try to access UCP, your browser warns that it doesn\u2019t trust UCP or that UCP has an invalid certificate. The same happens with other client tools. You can configure UCP to use your own TLS certificates, so that it is automatically trusted by your browser and client tools. Client Bundle A client bundle contains a private and public key pair that authorizes your requests in UCP. Using client certificate bundle on your local computer, you can use it to authenticate your requests. What is a client bundle? A client bundle is a group of certificates downloadable directly from the Docker Universal Control Plane (UCP) user interface within the admin section for \u201cMy Profile\u201d. This allows you to authorize a remote Docker engine to a specific user account managed in Docker EE, absorbing all associated RBAC controls in the process. You can now execute docker swarm commands from your remote machine that take effect on the remote cluster. https://www.docker.com/blog/get-familiar-docker-enterprise-edition-client-bundles/ Subjects A subject represents a user, team, or organization. A subject is granted a role for a collection of resources. These groups of users are the same across UCP and DTR making RBAC management across the entire software pipeline uniform. User - A single user or system account that an authentication backend (AD/LDAP) has validated. Team - A group of users that share a set of permissions defined in the team itself. A team exists only as part of an organization, and all team members are members of the organization. A team can exist in one organization only. Assign users to one or more teams and one or more organizations. Organization - The largest organizational unit in Docker Enterprise. Organizations group together teams to provide broader scope to apply access policy against. Subjects are individual users or teams within an organization. Teams are typically backed by an LDAP/AD group or search filter. It is also possible to add users manually. But it is not possible to have a hybrid composition of users. In other words, the list of users within a team should be derived from a directory server (e.g. AD) or should be added manually, not both. ucp-agent When you deploy UCP, it starts running a globally scheduled service called ucp-agent. This service monitors the node where it\u2019s running and starts and stops UCP services, based on whether the node is a manager or a worker node. Manager: the ucp-agent service automatically starts serving all UCP components, including the UCP web UI and data stores used by UCP. The ucp-agent accomplishes this by deploying several containers on the node. Worker: on worker nodes, the ucp-agent service starts serving a proxy service that ensures only authorized users and other UCP services can run Docker commands in that node. Bundles A client bundle contains a private and public key pair that authorizes your requests in UCP. In order to authenticate your requests, you can download client certificate bundle to your local computer. UCP issues different types of certificates depending on the user: User certificate bundles: only allow running docker commands through a UCP manager node. Admin user certificate bundles: allow running docker commands on the Docker Engine of any node. SAN Using external certificates is recommended when integrating with a corporate environment. Using external, officially-signed certificates simplifies having to distribute internal Certificate Authority (CA) certificates. One best practice is to use the Certificate Authority for your organization. Reduce the number of certificates by adding multiple Subject Alternative Names (SANs) to a single certificate. This allows the certificate to be valid for multiple URLs. For example, you can set up a certificate for ucp.example.com, dtr.example.com, and all the underlying hostnames and IP addresses. One certificate/key pair makes deploying certs easier. Docker CLI To use the Docker CLI with UCP, download a client certificate bundle(zip) by using the UCP web UI. Grant You can create by combining subject + role + resource set. A grant defines who has how much access to what resources. Each grant is a 1:1:1 mapping of subject, role, and resource set. For example, you can grant the \u201cProd Team\u201d \u201cRestricted Control\u201d over services in the \u201c/Production\u201d collection. x509 issue When you are running a docker command, Docker attempts to verify that the certificate in use is signed by UCP's certificate authority and that the domain name or IP used to connect to UCP is listed as a subject alternative name (SAN) in the UCP certificate. If you see the following error \u201cx509: certificate signed by unknown authority\u201d, then Docker has not been provided with the ca certificate. To resolve this issue, use a UCP client bundle to connect to UCP from the CLI. Node Certificate Expiration Universal Control Plane's management plane uses a private CA and certificates for all internal communication. The client certificates are automatically rotated on a schedule, providing a strong method for reducing the effect of a compromised node. There is an option to reduce the default time interval of 90 days to a shorter interval, however shorter intervals do add stress to the UCP cluster. To adjust the certificate rotation schedule, go to Admin -> Admin Settings -> Swarm and scroll down. Team's permission Permissions are cumulative. For example, if you have Write permissions, you automatically have Read permissions: - Read access allows users to view, search, and pull a private repository in the same way as they can a public repository. - Write access allows users to push to repositories on Docker Hub. - Admin access allows users to modify the repositories \u201cDescription\u201d, \u201cCollaborators\u201d rights, \u201cPublic/Private\u201d visibility, and \u201cDelete\u201d. RBAC UCP has role-based access control (RBAC), so that you can control who can access and make changes to your cluster and applications. Using the UI for Docker Enterprise of Docker Universal Control Plane (UCP), you authorize users to view, edit, and use cluster resources by granting role-based permissions against resource sets. To authorize access to cluster resources across your organization, UCP administrators might take the following high-level steps: - Add and configure subjects (users, teams, and service accounts). - Define custom roles (or use defaults) by adding permitted operations per type of resource. - Group cluster resources into resource sets of Swarm collections or Kubernetes namespaces. - Create grants by combining subject + role + resource set. roles The default roles in UCP are None, View Only, Restricted Control, Scheduler, and Full Control. Each of these roles have a set of operations that define the permissions associated with the role. Additional custom roles can be defined by combining a unique set of permissions. Custom roles can be leveraged to accommodate fine-grained access control as required for certain organizations and security controls. Backup Backups contain UCP configuration metadata to re-create configurations such as Administration Settings values such as LDAP and SAML, and RBAC configurations (Collections, Grants, Roles, User, and more): The following example shows how to create a UCP backup on a manager node, encrypt it by using a passphrase, decrypt it, verify its contents, and store it locally on the node at /tmp/mybackup.tar: $ docker container run \\ --rm \\ --log-driver none \\ --name ucp \\ --volume /var/run/docker.sock:/var/run/docker.sock \\ --volume /tmp:/backup \\ docker/ucp:3.2.6 backup \\ --file mybackup.tar \\ --passphrase \"secret12chars\" \\ --include-logs=false Here, the \u201cdocker/ucp\u201d represents a docker image, \u201cbackup\u201d is the command to execute. #_ping You can use the https:// /_ping endpoint to check the health of a single UCP manager node. When you access this endpoint, the UCP manager validates that all its internal components are working, and returns one of the following HTTP error codes: 200, if all components are healthy 500, if one or more components are not healthy SSO with UCP To only authenticate once, you can configure DTR to have single sign-on (SSO) with UCP. Users are shared between UCP and DTR by default, but the applications have separate browser-based interfaces which require authentication. When installing DTR, pass --dtr-external-url to enable SSO. You can also enable single sign-on from the command line by reconfiguring your DTR. To do so, run the following: docker run --rm -it \\ docker/dtr:2.7.6 reconfigure \\ --dtr-external-url dtr.example.com \\ \u2026 Which role should be set in UCP for the DBA team to allow them to create their own volumes? View Only If the type of orchestrator is changed, the existing workload will not migrate to the new orchestrator automatically. docker node update \\ --label-add com.docker.ucp.orchestrator.swarm=true Ucp-agent - core component of UCP ucp-dsinfo - Docker system information collection script to assist with troubleshooting TCP 179 port for managers, workers commands is used to access audit logs of Universal Control Plane docker logs ucp-controller","title":"Universal Control Plane"},{"location":"DockerEE/ucp/#ucp","text":"UCP always runs with HTTPS enabled. When you connect to UCP, you need to make sure that the hostname that you use to connect is recognized by UCP\u2019s certificates. If, for instance, you put UCP behind a load balancer that forwards its traffic to your UCP instance, your requests will be for the load balancer\u2019s hostname or IP address, not UCP\u2019s. UCP will reject these requests unless you include the load balancer\u2019s address as a Subject Alternative Name (or SAN) in UCP\u2019s certificates. If you use your own TLS certificates, make sure that they have the correct SAN values If you want to use the self-signed certificate that UCP has out of the box, you can set up the SANs when you install UCP with the --san argument. You can also add them after installation. Sign images using notary - To sign images in a way that UCP trusts them you need to: - Configure your Notary client - Initialize trust metadata for the repository - Delegate signing to the keys in your UCP client bundle","title":"UCP"},{"location":"DockerEE/ucp/#collections","text":"Docker Enterprise enables controlling access to swarm resources by using collections. A collection is a grouping of swarm cluster resources that you access by specifying a directory-like path. Before grants can be implemented, collections need to be designed to group resources in a way that makes sense for an organization. The following example shows the potential access policy of an organization. Consider an organization with two application teams, Mobile and Payments, that share cluster hardware resources, but still need to segregate access to the applications. Collections should be designed to map to the organizational structure desired, in this case the two application teams. Collections are implemented in UCP through the use of Docker labels. All resources within a given collection are labeled with the collection, /production/mobile for instance. Collections are groupings of objects within UCP. A collection can be made up of one or many of nodes, stacks, containers, services, volumes, networks, secrets, or configs \u2014 or it can hold other collections. To associate a node or a stack or any resource with a collection, that resource should share the label com.docker.ucp.access.label with the collection. A resource can be associated with zero or multiple collections, and a collection can have zero or multiple resources or other child collections in it. Collections within collections allow the structuring of resource objects in a hierarchical nature and can significantly simplify access control. Access provided at a top level collection is inherited by all its children, including any child collections.","title":"Collections"},{"location":"DockerEE/ucp/#ldap","text":"Docker UCP integrates with LDAP directory services, so that you can manage users and groups from your organization\u2019s directory and it will automatically propagate that information to UCP and DTR. UCP services include a component called ucp-auth-api, which is the centralized service for identity and authentication used by UCP and DTR. All UCP services are exposed using HTTPS, to ensure all communications between clients and UCP are encrypted. By default, this is done using self-signed TLS certificates that are not trusted by client tools like web browsers. So when you try to access UCP, your browser warns that it doesn\u2019t trust UCP or that UCP has an invalid certificate. The same happens with other client tools. You can configure UCP to use your own TLS certificates, so that it is automatically trusted by your browser and client tools.","title":"LDAP"},{"location":"DockerEE/ucp/#client-bundle","text":"A client bundle contains a private and public key pair that authorizes your requests in UCP. Using client certificate bundle on your local computer, you can use it to authenticate your requests. What is a client bundle? A client bundle is a group of certificates downloadable directly from the Docker Universal Control Plane (UCP) user interface within the admin section for \u201cMy Profile\u201d. This allows you to authorize a remote Docker engine to a specific user account managed in Docker EE, absorbing all associated RBAC controls in the process. You can now execute docker swarm commands from your remote machine that take effect on the remote cluster. https://www.docker.com/blog/get-familiar-docker-enterprise-edition-client-bundles/","title":"Client Bundle"},{"location":"DockerEE/ucp/#subjects","text":"A subject represents a user, team, or organization. A subject is granted a role for a collection of resources. These groups of users are the same across UCP and DTR making RBAC management across the entire software pipeline uniform. User - A single user or system account that an authentication backend (AD/LDAP) has validated. Team - A group of users that share a set of permissions defined in the team itself. A team exists only as part of an organization, and all team members are members of the organization. A team can exist in one organization only. Assign users to one or more teams and one or more organizations. Organization - The largest organizational unit in Docker Enterprise. Organizations group together teams to provide broader scope to apply access policy against. Subjects are individual users or teams within an organization. Teams are typically backed by an LDAP/AD group or search filter. It is also possible to add users manually. But it is not possible to have a hybrid composition of users. In other words, the list of users within a team should be derived from a directory server (e.g. AD) or should be added manually, not both.","title":"Subjects"},{"location":"DockerEE/ucp/#ucp-agent","text":"When you deploy UCP, it starts running a globally scheduled service called ucp-agent. This service monitors the node where it\u2019s running and starts and stops UCP services, based on whether the node is a manager or a worker node. Manager: the ucp-agent service automatically starts serving all UCP components, including the UCP web UI and data stores used by UCP. The ucp-agent accomplishes this by deploying several containers on the node. Worker: on worker nodes, the ucp-agent service starts serving a proxy service that ensures only authorized users and other UCP services can run Docker commands in that node.","title":"ucp-agent"},{"location":"DockerEE/ucp/#bundles","text":"A client bundle contains a private and public key pair that authorizes your requests in UCP. In order to authenticate your requests, you can download client certificate bundle to your local computer. UCP issues different types of certificates depending on the user: User certificate bundles: only allow running docker commands through a UCP manager node. Admin user certificate bundles: allow running docker commands on the Docker Engine of any node.","title":"Bundles"},{"location":"DockerEE/ucp/#san","text":"Using external certificates is recommended when integrating with a corporate environment. Using external, officially-signed certificates simplifies having to distribute internal Certificate Authority (CA) certificates. One best practice is to use the Certificate Authority for your organization. Reduce the number of certificates by adding multiple Subject Alternative Names (SANs) to a single certificate. This allows the certificate to be valid for multiple URLs. For example, you can set up a certificate for ucp.example.com, dtr.example.com, and all the underlying hostnames and IP addresses. One certificate/key pair makes deploying certs easier.","title":"SAN"},{"location":"DockerEE/ucp/#docker-cli","text":"To use the Docker CLI with UCP, download a client certificate bundle(zip) by using the UCP web UI.","title":"Docker CLI"},{"location":"DockerEE/ucp/#grant","text":"You can create by combining subject + role + resource set. A grant defines who has how much access to what resources. Each grant is a 1:1:1 mapping of subject, role, and resource set. For example, you can grant the \u201cProd Team\u201d \u201cRestricted Control\u201d over services in the \u201c/Production\u201d collection.","title":"Grant"},{"location":"DockerEE/ucp/#x509-issue","text":"When you are running a docker command, Docker attempts to verify that the certificate in use is signed by UCP's certificate authority and that the domain name or IP used to connect to UCP is listed as a subject alternative name (SAN) in the UCP certificate. If you see the following error \u201cx509: certificate signed by unknown authority\u201d, then Docker has not been provided with the ca certificate. To resolve this issue, use a UCP client bundle to connect to UCP from the CLI.","title":"x509 issue"},{"location":"DockerEE/ucp/#node-certificate-expiration","text":"Universal Control Plane's management plane uses a private CA and certificates for all internal communication. The client certificates are automatically rotated on a schedule, providing a strong method for reducing the effect of a compromised node. There is an option to reduce the default time interval of 90 days to a shorter interval, however shorter intervals do add stress to the UCP cluster. To adjust the certificate rotation schedule, go to Admin -> Admin Settings -> Swarm and scroll down.","title":"Node Certificate Expiration"},{"location":"DockerEE/ucp/#teams-permission","text":"Permissions are cumulative. For example, if you have Write permissions, you automatically have Read permissions: - Read access allows users to view, search, and pull a private repository in the same way as they can a public repository. - Write access allows users to push to repositories on Docker Hub. - Admin access allows users to modify the repositories \u201cDescription\u201d, \u201cCollaborators\u201d rights, \u201cPublic/Private\u201d visibility, and \u201cDelete\u201d.","title":"Team's permission"},{"location":"DockerEE/ucp/#rbac","text":"UCP has role-based access control (RBAC), so that you can control who can access and make changes to your cluster and applications. Using the UI for Docker Enterprise of Docker Universal Control Plane (UCP), you authorize users to view, edit, and use cluster resources by granting role-based permissions against resource sets. To authorize access to cluster resources across your organization, UCP administrators might take the following high-level steps: - Add and configure subjects (users, teams, and service accounts). - Define custom roles (or use defaults) by adding permitted operations per type of resource. - Group cluster resources into resource sets of Swarm collections or Kubernetes namespaces. - Create grants by combining subject + role + resource set.","title":"RBAC"},{"location":"DockerEE/ucp/#roles","text":"The default roles in UCP are None, View Only, Restricted Control, Scheduler, and Full Control. Each of these roles have a set of operations that define the permissions associated with the role. Additional custom roles can be defined by combining a unique set of permissions. Custom roles can be leveraged to accommodate fine-grained access control as required for certain organizations and security controls.","title":"roles"},{"location":"DockerEE/ucp/#backup","text":"Backups contain UCP configuration metadata to re-create configurations such as Administration Settings values such as LDAP and SAML, and RBAC configurations (Collections, Grants, Roles, User, and more): The following example shows how to create a UCP backup on a manager node, encrypt it by using a passphrase, decrypt it, verify its contents, and store it locally on the node at /tmp/mybackup.tar: $ docker container run \\ --rm \\ --log-driver none \\ --name ucp \\ --volume /var/run/docker.sock:/var/run/docker.sock \\ --volume /tmp:/backup \\ docker/ucp:3.2.6 backup \\ --file mybackup.tar \\ --passphrase \"secret12chars\" \\ --include-logs=false Here, the \u201cdocker/ucp\u201d represents a docker image, \u201cbackup\u201d is the command to execute.","title":"Backup"},{"location":"DockerEE/ucp/#_ping","text":"You can use the https:// /_ping endpoint to check the health of a single UCP manager node. When you access this endpoint, the UCP manager validates that all its internal components are working, and returns one of the following HTTP error codes: 200, if all components are healthy 500, if one or more components are not healthy","title":"#_ping"},{"location":"DockerEE/ucp/#sso-with-ucp","text":"To only authenticate once, you can configure DTR to have single sign-on (SSO) with UCP. Users are shared between UCP and DTR by default, but the applications have separate browser-based interfaces which require authentication. When installing DTR, pass --dtr-external-url to enable SSO. You can also enable single sign-on from the command line by reconfiguring your DTR. To do so, run the following: docker run --rm -it \\ docker/dtr:2.7.6 reconfigure \\ --dtr-external-url dtr.example.com \\ \u2026","title":"SSO with UCP"},{"location":"DockerEE/ucp/#which-role-should-be-set-in-ucp-for-the-dba-team-to-allow-them-to-create-their-own-volumes-view-only","text":"If the type of orchestrator is changed, the existing workload will not migrate to the new orchestrator automatically. docker node update \\ --label-add com.docker.ucp.orchestrator.swarm=true Ucp-agent - core component of UCP ucp-dsinfo - Docker system information collection script to assist with troubleshooting TCP 179 port for managers, workers","title":"Which role should be set in UCP for the DBA team to allow them to create their own volumes? View Only"},{"location":"DockerEE/ucp/#commands-is-used-to-access-audit-logs-of-universal-control-plane","text":"docker logs ucp-controller","title":"commands is used to access audit logs of Universal Control Plane"},{"location":"Networking/bridge/","text":"Create the Bridge Network Create a bridge network called prices-net. docker network create --driver bridge prices-net Create the base-price Container Create a container for the component that serves base prices. docker run --name base-price -d --network prices-net linuxacademycontent/prices-base-price:1 Create the sales Container Create a container for the component that serves products on sale. docker run --name sales -d --network prices-net linuxacademycontent/prices-sales:1 Create the total-price Container Create a container for the component that serves the total prices of products. docker run --name total-price -d --network prices-net -p 8080:80 linuxacademycontent/prices-total-price:1 Verify that everything is set up correctly. curl localhost:8080 You should get a list of products and their final prices. The total-prices container calculates these prices by first querying the other two containers. This communication takes place over the prices-net bridge network.","title":"Bridge"},{"location":"Networking/concepts/","text":"Container Networking Model There are several high-level constructs in the CNM. They are all OS and infrastructure agnostic so that applications can have a uniform experience no matter the infrastructure stack. Sandbox \u2014 A Sandbox contains the configuration of a container's network stack. This includes the management of the container's interfaces, routing table, and DNS settings. An implementation of a Sandbox could be a Windows HNS or Linux Network Namespace, a FreeBSD Jail, or other similar concept. A Sandbox may contain many endpoints from multiple networks. Endpoint \u2014 An Endpoint joins a Sandbox to a Network. The Endpoint construct exists so the actual connection to the network can be abstracted away from the application. This helps maintain portability so that a service can use different types of network drivers without being concerned with how it's connected to that network. Network \u2014 The CNM does not specify a Network in terms of the OSI model. An implementation of a Network could be a Linux bridge, a VLAN, etc. A Network is a collection of endpoints that have connectivity between them. Endpoints that are not connected to a network do not have connectivity on a network. The sandbox perfectly isolates a container from the outside world. No inbound network connection is allowed into the sandboxed container. Yet, it is very unlikely that a container will be of any value in a system if absolutely no communication with it is possible. To work around this, we have element number two, which is the endpoint. An endpoint is a controlled gateway from the outside world into the network's sandbox that shields the container. The endpoint connects the network sandbox, but not the container, to the third element of the model, which is the network. Also, the Endpoint construct exists so the actual connection to the network can be abstracted away from the application. This helps maintain portability so that a service can use different types of network drivers without being concerned with how it's connected to that network. User-defined bridges User-defined bridges provide automatic DNS resolution between containers. User-defined bridges provide better isolation. Containers can be attached and detached from user-defined networks on the fly. Each user-defined network creates a configurable bridge. Linked containers on the default bridge network share environment variables. Networking commandsd --publish-all or -P Publish all exposed ports to the host interfaces. Docker binds each exposed port to a random port on the host. Use the -p flag to explicitly map a single port or range of ports. -p or --publish It makes a port available to services outside of Docker, or to Docker containers which are not connected to the container\u2019s network. format: ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort | containerPort IPAM Drivers Docker has a native IP Address Management Driver that provides default subnets or IP addresses for the networks and endpoints if they are not specified. IP addressing can also be manually assigned through network, container, and service create commands. Remote IPAM drivers also exist and provide integration to existing IPAM tools. Encrypt traffic on an overlay network All swarm service management traffic is encrypted by default, using the AES algorithm in GCM mode. Manager nodes in the swarm rotate the key used to encrypt gossip data every 12 hours. To encrypt application data as well, add --opt encrypted when creating the overlay network. This enables IPSEC encryption at the level of the vxlan. This encryption imposes a non-negligible performance penalty, so you should test this option before using it in production. https://docs.docker.com/network/overlay/ To set driver specific options use: --opt , -o For example: $ docker network create -d overlay \\ --opt encrypted=true \\ my-network -network https://docs.docker.com/engine/reference/commandline/network_create/ Network set to host With the network set to host a container will share the host\u2019s network stack and all interfaces from the host will be available to the container. The container\u2019s hostname will match the hostname on the host system. Even in host network mode a container has its own UTS namespace by default. As such --hostname and --domainname are allowed in host network mode and will only change the hostname and domain name inside the container. https://docs.docker.com/engine/reference/run/#network-settings Macvlan Network When you create a macvlan network, it can either be in bridge mode or 802.1q trunk bridge mode. In bridge mode, macvlan traffic goes through a physical device on the host. In 802.1q trunk bridge mode, traffic goes through an 802.1q sub-interface which Docker creates on the fly. This allows you to control routing and filtering at a more granular level. https://docs.docker.com/network/macvlan/#create-a-macvlan-network Linux Bridges vs VXLAN Linux bridges only operate locally and cannot span across nodes. So, for multi-host networking we need another mechanism. Linux VXLAN comes to the rescue. When a container sends a data packet, the bridge realizes that the target of the packet is not on this host. Now, each node participating in an overlay network gets a so-called VXLAN Tunnel Endpoint (VTEP) object, which intercepts the packet (the packet at that moment is an OSI layer 2 data packet), wraps it with a header containing the target IP address of the host that runs the target container (this makes it now an OSI layer 3 data packet), and sends it over the VXLAN tunnel. The VTEP on the other side of the tunnel unpacks the data packet and forwards it to the local bridge, which in turn forwards it to the target container. Create overlay network To create an overlay network which can be used by swarm services or standalone containers to communicate with other standalone containers running on other Docker daemons, add the --attachable flag: $ docker network create -d overlay --attachable my-attachable-overlay Nodeport vs Ingress NodePort service type is well suited for application services that require direct TCP/UDP access, like RabbitMQ for example. If your service can be accessed using HTTP/HTTPs then Ingress is recommended. Ingress is a Kubernetes API object that manages external access to the services in a cluster, typically HTTP/HTTPS. Host network If you use the host network mode for a container, that container\u2019s network stack is not isolated from the Docker host (the container shares the host\u2019s networking namespace), and the container does not get its own IP-address allocated. For instance, if you run a container which binds to port 80 and you use host networking, the container\u2019s application is available on port 80 on the host\u2019s IP address. The host networking driver only works on Linux hosts, and is not supported on Docker Desktop for Mac, Docker Desktop for Windows, or Docker EE for Windows Server. Host mode networking can be useful to optimize performance, and in situations where a container needs to handle a large range of ports, as it does not require network address translation (NAT), and no \u201cuserland-proxy\u201d is created for each port. https://docs.docker.com/network/host/ Which of the following sentences are true about overlay networks? a) DTR deploys an overlay network, dtr-ol, to route a cluster's internal communications. b) Overlay-defined networks are only present on manager nodes when there is not a task connected to them. c) interlock-extension connects to services' defined networks to route requests to appropriate backends. The overlay2 driver natively supports up to 128 lower OverlayFS layers. This capability provides better performance for layer-related Docker commands such as docker build and docker commit, and consumes fewer inodes on the backing filesystem.","title":"Concepts"},{"location":"Networking/concepts/#container-networking-model","text":"There are several high-level constructs in the CNM. They are all OS and infrastructure agnostic so that applications can have a uniform experience no matter the infrastructure stack. Sandbox \u2014 A Sandbox contains the configuration of a container's network stack. This includes the management of the container's interfaces, routing table, and DNS settings. An implementation of a Sandbox could be a Windows HNS or Linux Network Namespace, a FreeBSD Jail, or other similar concept. A Sandbox may contain many endpoints from multiple networks. Endpoint \u2014 An Endpoint joins a Sandbox to a Network. The Endpoint construct exists so the actual connection to the network can be abstracted away from the application. This helps maintain portability so that a service can use different types of network drivers without being concerned with how it's connected to that network. Network \u2014 The CNM does not specify a Network in terms of the OSI model. An implementation of a Network could be a Linux bridge, a VLAN, etc. A Network is a collection of endpoints that have connectivity between them. Endpoints that are not connected to a network do not have connectivity on a network. The sandbox perfectly isolates a container from the outside world. No inbound network connection is allowed into the sandboxed container. Yet, it is very unlikely that a container will be of any value in a system if absolutely no communication with it is possible. To work around this, we have element number two, which is the endpoint. An endpoint is a controlled gateway from the outside world into the network's sandbox that shields the container. The endpoint connects the network sandbox, but not the container, to the third element of the model, which is the network. Also, the Endpoint construct exists so the actual connection to the network can be abstracted away from the application. This helps maintain portability so that a service can use different types of network drivers without being concerned with how it's connected to that network.","title":"Container Networking Model"},{"location":"Networking/concepts/#user-defined-bridges","text":"User-defined bridges provide automatic DNS resolution between containers. User-defined bridges provide better isolation. Containers can be attached and detached from user-defined networks on the fly. Each user-defined network creates a configurable bridge. Linked containers on the default bridge network share environment variables.","title":"User-defined bridges"},{"location":"Networking/concepts/#networking-commandsd","text":"--publish-all or -P Publish all exposed ports to the host interfaces. Docker binds each exposed port to a random port on the host. Use the -p flag to explicitly map a single port or range of ports. -p or --publish It makes a port available to services outside of Docker, or to Docker containers which are not connected to the container\u2019s network. format: ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort | containerPort","title":"Networking commandsd"},{"location":"Networking/concepts/#ipam-drivers","text":"Docker has a native IP Address Management Driver that provides default subnets or IP addresses for the networks and endpoints if they are not specified. IP addressing can also be manually assigned through network, container, and service create commands. Remote IPAM drivers also exist and provide integration to existing IPAM tools.","title":"IPAM Drivers"},{"location":"Networking/concepts/#encrypt-traffic-on-an-overlay-network","text":"All swarm service management traffic is encrypted by default, using the AES algorithm in GCM mode. Manager nodes in the swarm rotate the key used to encrypt gossip data every 12 hours. To encrypt application data as well, add --opt encrypted when creating the overlay network. This enables IPSEC encryption at the level of the vxlan. This encryption imposes a non-negligible performance penalty, so you should test this option before using it in production. https://docs.docker.com/network/overlay/ To set driver specific options use: --opt , -o For example: $ docker network create -d overlay \\ --opt encrypted=true \\ my-network -network https://docs.docker.com/engine/reference/commandline/network_create/","title":"Encrypt traffic on an overlay network"},{"location":"Networking/concepts/#network-set-to-host","text":"With the network set to host a container will share the host\u2019s network stack and all interfaces from the host will be available to the container. The container\u2019s hostname will match the hostname on the host system. Even in host network mode a container has its own UTS namespace by default. As such --hostname and --domainname are allowed in host network mode and will only change the hostname and domain name inside the container. https://docs.docker.com/engine/reference/run/#network-settings","title":"Network set to host"},{"location":"Networking/concepts/#macvlan-network","text":"When you create a macvlan network, it can either be in bridge mode or 802.1q trunk bridge mode. In bridge mode, macvlan traffic goes through a physical device on the host. In 802.1q trunk bridge mode, traffic goes through an 802.1q sub-interface which Docker creates on the fly. This allows you to control routing and filtering at a more granular level. https://docs.docker.com/network/macvlan/#create-a-macvlan-network","title":"Macvlan Network"},{"location":"Networking/concepts/#linux-bridges-vs-vxlan","text":"Linux bridges only operate locally and cannot span across nodes. So, for multi-host networking we need another mechanism. Linux VXLAN comes to the rescue. When a container sends a data packet, the bridge realizes that the target of the packet is not on this host. Now, each node participating in an overlay network gets a so-called VXLAN Tunnel Endpoint (VTEP) object, which intercepts the packet (the packet at that moment is an OSI layer 2 data packet), wraps it with a header containing the target IP address of the host that runs the target container (this makes it now an OSI layer 3 data packet), and sends it over the VXLAN tunnel. The VTEP on the other side of the tunnel unpacks the data packet and forwards it to the local bridge, which in turn forwards it to the target container.","title":"Linux Bridges vs VXLAN"},{"location":"Networking/concepts/#create-overlay-network","text":"To create an overlay network which can be used by swarm services or standalone containers to communicate with other standalone containers running on other Docker daemons, add the --attachable flag: $ docker network create -d overlay --attachable my-attachable-overlay","title":"Create overlay network"},{"location":"Networking/concepts/#nodeport-vs-ingress","text":"NodePort service type is well suited for application services that require direct TCP/UDP access, like RabbitMQ for example. If your service can be accessed using HTTP/HTTPs then Ingress is recommended. Ingress is a Kubernetes API object that manages external access to the services in a cluster, typically HTTP/HTTPS.","title":"Nodeport vs Ingress"},{"location":"Networking/concepts/#host-network","text":"If you use the host network mode for a container, that container\u2019s network stack is not isolated from the Docker host (the container shares the host\u2019s networking namespace), and the container does not get its own IP-address allocated. For instance, if you run a container which binds to port 80 and you use host networking, the container\u2019s application is available on port 80 on the host\u2019s IP address. The host networking driver only works on Linux hosts, and is not supported on Docker Desktop for Mac, Docker Desktop for Windows, or Docker EE for Windows Server. Host mode networking can be useful to optimize performance, and in situations where a container needs to handle a large range of ports, as it does not require network address translation (NAT), and no \u201cuserland-proxy\u201d is created for each port. https://docs.docker.com/network/host/","title":"Host network"},{"location":"Networking/concepts/#which-of-the-following-sentences-are-true-about-overlay-networks","text":"a) DTR deploys an overlay network, dtr-ol, to route a cluster's internal communications. b) Overlay-defined networks are only present on manager nodes when there is not a task connected to them. c) interlock-extension connects to services' defined networks to route requests to appropriate backends. The overlay2 driver natively supports up to 128 lower OverlayFS layers. This capability provides better performance for layer-related Docker commands such as docker build and docker commit, and consumes fewer inodes on the backing filesystem.","title":"Which of the following sentences are true about overlay networks?"},{"location":"Networking/external-dns/","text":"$ docker run --dns 10.0.0.2 busybox nslookup google.com or edit your /etc/docker/daemon.json to have something like: { \"dns\": [\"10.0.0.2\", \"8.8.8.8\"] } then restart docker service $ sudo systemctl docker restart","title":"External DNS"},{"location":"Networking/swarm/","text":"Docker Swarm By default, Docker Swarm uses a default address pool 10.0.0.0/8 for global scope (overlay) networks. Every network that does not have a subnet specified will have a subnet sequentially allocated from this pool. In some circumstances it may be desirable to use a different default IP address pool for networks. For example, if the default 10.0.0.0/8 range conflicts with already allocated address space in your network, then it is desirable to ensure that networks use a different range without requiring Swarm users to specify each subnet with the --subnet command. https://docs.docker.com/engine/swarm/swarm-mode/#configuring-default-address-pools Docker Swarm Task states Create a service by using docker service create. The request goes to a Docker manager node. The Docker manager node schedules the service to run on particular nodes. Each service can start multiple tasks. Each task has a life cycle, with states like NEW, PENDING, and COMPLETE. There is no limit on the number of manager nodes. The decision about how many manager nodes to implement is a trade-off between performance and fault-tolerance. Adding manager nodes to a swarm makes the swarm more fault-tolerant. However, additional manager nodes reduce write performance because more nodes must acknowledge proposals to update the swarm state. This means more network round-trip traffic. Raft consensus in swarm mode The reason why Docker swarm mode is using a consensus algorithm is to make sure that all the manager nodes that are in charge of managing and scheduling tasks in the cluster, are storing the same consistent state. Raft tolerates up to (N-1)/2 failures and requires a majority or quorum of (N/2)+1 members to agree on values proposed to the cluster. Docker Service Discovery Docker uses embedded DNS to provide service discovery for containers running on a single Docker engine and tasks running in a Docker swarm. The Docker engine checks if the DNS query belongs to a container or service on each network that the requesting container belongs to. If it does, then the Docker engine looks up the IP address that matches the name of a container, task, or service in a key-value store and returns that IP or service Virtual IP (VIP) back to the requester. Here, all nodes participate in a network called routing mesh. https://success.docker.com/article/ucp-service-discovery-swarm DNS round robin (DNS RR) load balancing is another load balancing option for services (configured with --endpoint-mode dnsrr). In DNS RR mode a VIP is not created for each service. The Docker DNS server resolves a service name to individual container IPs in round robin fashion. https://success.docker.com/article/networking To use an external load balancer without the routing mesh, set --endpoint-mode to dnsrr instead of the default value of vip. In this case, there is not a single virtual IP. Instead, Docker sets up DNS entries for the service such that a DNS query for the service name returns a list of IP addresses, and the client connects directly to one of these. You are responsible for providing the list of IP addresses and ports to your load balancer. See Configure service discovery. https://docs.docker.com/engine/swarm/ingress/#without-the-routing-mesh Services using the routing mesh are running in virtual IP (VIP) mode. Even a service running on each node (by means of the --mode global flag) uses the routing mesh. When using the routing mesh, there is no guarantee about which Docker node services client requests. To bypass the routing mesh, you can start a service using DNS Round Robin (DNSRR) mode, by setting the --endpoint-mode flag to dnsrr. https://docs.docker.com/network/overlay/#bypass-the-routing-mesh-for-a-swarm-service Routing Mesh vs Host Mode When you create a swarm service, you can publish that service\u2019s ports to hosts outside the swarm in two ways: You can rely on the routing mesh. When you publish a service port, the swarm makes the service accessible at the target port on every node, regardless of whether there is a task for the service running on that node or not. This is less complex and is the right choice for many types of services. You can publish a service task\u2019s port directly on the swarm node where that service is running. This feature is available in Docker 1.13 and higher. This bypasses the routing mesh and provides the maximum flexibility, including the ability for you to develop your own routing framework. However, you are responsible for keeping track of where each task is running and routing requests to the tasks, and load-balancing across the nodes. By default, services are using the routing mesh which makes the service accessible at the published port on every swarm node. When a user or process connects to a service, any worker node running a service task may respond. To publish a service\u2019s port directly on the node where it is running, use the mode=host option to the --publish flag. For example: $ docker service create \\ --mode global \\ --publish mode=host,target=80,published=8080 \\ --name=nginx \\ nginx:latest https://docs.docker.com/engine/swarm/services/#publish-ports Concepts By default Docker Swarm uses a default address pool 10.0.0.0/8 for global scope (overlay) networks. If --default-addr-pool-mask-len were unspecified or set explicitly to 24, this would result in 256 /24 networks of the form 10.10.X.0/24. Demote manager node to worker node and docker swarm leave Lock your swam to protect its encryption key You don\u2019t need to unlock the swarm when a new node joins the swarm, because the key is propagated to it over mutual TLS. When Docker restarts, you must unlock the swarm first, using a key encryption key generated by Docker when the swarm was locked. You can rotate this key encryption key at any time. When you rotate the unlock key, keep a record of the old key around for a few minutes, so that if a manager goes down before it gets the new key, it may still be unlocked with the old one. Worker node can join the cluster without the need of autolock key A node is an instance of the Docker engine participating in the swarm. Setting a node to DRAIN does not remove standalone containers from that node, such as those created with docker run, docker-compose up, or the Docker Engine API. A node\u2019s status, including DRAIN, only affects the node\u2019s ability to schedule swarm service workloads. https://docs.docker.com/engine/swarm/swarm-tutorial/drain-node/ DRAIN availability prevents a node from receiving new tasks from the swarm manager. It also means the manager stops tasks running on the node and launches replica tasks on a node with ACTIVE availability. https://docs.docker.com/engine/swarm/swarm-tutorial/drain-node/ Sometimes, such as planned maintenance times, you need to set a node to DRAIN availability. DRAIN availability prevents a node from receiving new tasks from the swarm manager. It also means the manager stops tasks running on the node and launches replica tasks on a node with ACTIVE availability. Docker manager nodes store the swarm state and manager logs in the /var/lib/docker/swarm/ directory. Usually, on Linux distributions: \"/etc\" is used for configurations (.conf files etc). here you find all the configs and settings for your system. \"/var\" is usually used for log files, 'temporary' files (like mail spool, printer spool, etc), databases, and all other data not tied to a specific user. Logs are usually in \"/var/log\", databases in \"/var/lib\" (mysql - \"/var/lib/mysql\"), etc. Ingress When you create a swarm service and do not connect it to a user-defined overlay network, it connects to the ingress network by default. The ingress network is created without the --attachable flag, which means that only swarm services can use it, and not standalone containers. You can connect standalone containers to user-defined overlay networks which are created with the --attachable flag. https://docs.docker.com/network/overlay/#attach-a-standalone-container-to-an-overlay-network The routing mesh allows all the swarm nodes to accept connections on the services published ports. When any swarm node receives traffic destined to the published TCP/UDP port of a running service, it forwards the traffic to the service's VIP using a pre-defined overlay network called ingress. The ingress network behaves similarly to other overlay networks, but its sole purpose is to transport mesh routing traffic from external clients to cluster services. It uses the same VIP-based internal load balancing as described in the previous section. https://success.docker.com/article/ucp-service-discovery-swarm#interlockproxyusageexamples Docker Engine swarm mode makes it easy to publish ports for services to make them available to resources outside the swarm. All nodes participate in an ingress routing mesh. The routing mesh enables each node in the swarm to accept connections on published ports for any service running in the swarm, even if there\u2019s no task running on the node. The routing mesh routes all incoming requests to published ports on available nodes to an active container. https://docs.docker.com/engine/swarm/ingress/ Initialise a swarm When you initialize a swarm or join a Docker host to an existing swarm, two new networks are created on that Docker host: - an overlay network called ingress, which handles control and data traffic related to swarm services. When you create a swarm service and do not connect it to a user-defined overlay network, it connects to the ingress network by default. - a bridge network called docker_gwbridge, which connects the individual Docker daemon to the other daemons participating in the swarm. autolock Docker 1.13 introduces the ability to protect the mutual TLS encryption key and the key used to encrypt and decrypt secrets, by allowing you to take ownership of these keys and to require manual unlocking of your managers. This feature is called autolock. When Docker restarts, you must unlock the swarm first, using a key encryption key generated by Docker when the swarm was locked. You can rotate this key encryption key at any time. To enable autolock on an existing swarm, set the autolock flag to true. docker swarm update --autolock=true https://docs.docker.com/engine/swarm/swarm_manager_locking/#enable-or-disable-autolock-on-an-existing-swarm Secrets Sensitive information to containers running on a Swarm are normally stored in a secret. A secret (usually containing credentials, certificates, and other private information) is provided to service at runtime. The secret is saved in the Raft logs. The Raft logs used by swarm managers are encrypted on disk by default. When Docker restarts, both the TLS key used to encrypt communication among swarm nodes, and the key used to encrypt and decrypt Raft logs on disk, are loaded into each manager node\u2019s memory. Docker 1.13 introduces the ability to protect the mutual TLS encryption key and the key used to encrypt and decrypt Raft logs at rest, by allowing you to take ownership of these keys and to require manual unlocking of your managers. This feature is called autolock. When Docker restarts, you must unlock the swarm first, using a key encryption key generated by Docker when the swarm was locked. You can rotate this key encryption key at any time. When you initialize a new swarm, you can use the --autolock flag to enable autolocking of swarm manager nodes when Docker restarts. docker swarm init --autolock To enable autolock on an existing swarm, set the autolock flag to true. docker swarm update --autolock=true In terms of Docker Swarm services, a secret is a blob of data, such as a password, SSH private key, SSL certificate, or another piece of data that should not be transmitted over a network or stored unencrypted in a Dockerfile or in your application\u2019s source code. https://docs.docker.com/engine/swarm/secrets/#about-secrets The command to create a secret from a file or standard input (STDIN) as content is: docker secret create [OPTIONS] SECRET [file|-] Since this is a cluster management command, it must be executed on a swarm manager node. https://docs.docker.com/engine/reference/commandline/secret_create/ Note: After you create a secret, you cannot update it. You can only remove and re-create it, and you cannot remove a secret that a service is using. https://docs.docker.com/engine/swarm/secrets/#advanced-example-use-secrets-with-a-wordpress-service Secrets are encrypted during transit and at rest in a Docker swarm (The secret is stored in the Raft log, which is encrypted) A Docker secret is a blob of sensitive data that should not be transmitted over a network, such as: - Usernames and passwords - TLS certificates and keys - SSH keys - Other important data such as the name of a database or internal server - Generic strings or binary content (up to 500 kb in size) Secret files with custom targets are not directly bind-mounted into Windows containers, since Windows does not support non-directory file bind-mounts. Instead, secrets for a container are all mounted in C:\\ProgramData\\Docker\\internal\\secrets (an implementation detail which should not be relied upon by applications) within the container. Symbolic links are used to point from there to the desired target of the secret within the container. The default target is C:\\ProgramData\\Docker\\secrets. The location of the mount point within the container defaults to /run/secrets/ in Linux containers, or C:\\ProgramData\\Docker\\secrets in Windows containers. You can also specify a custom location. Add or remove secrets Use the --secret-add or --secret-rm options add or remove a service\u2019s secrets. For example: docker service update \\ --secret-rm mysql_password \\ --secret-add source=mysql_password_v2,target=wp_db_password,mode=0400 \\ wordpress This triggers a rolling restart of the WordPress service and the new secret is used. The following example adds a secret named ssh-2 and removes ssh-1: $ docker service update \\ --secret-add source=ssh-2,target=ssh-2 \\ --secret-rm ssh-1 \\ Myservice docker swarm init When you create a swarm by running docker swarm init, Docker designates itself as a manager node. By default, the manager node generates a new root Certificate Authority (CA) along with a key pair, which are used to secure communications with other nodes that join the swarm. If you prefer, you can specify your own externally-generated root CA, using the --external-ca flag of the docker swarm init command. PKI The swarm mode public key infrastructure (PKI) system built into Docker makes it simple to securely deploy a container orchestration system. The nodes in a swarm use mutual Transport Layer Security (TLS) to authenticate, authorize, and encrypt the communications with other nodes in the swarm. Rotating the CA certificate In the event that a cluster CA key or a manager node is compromised, you can rotate the swarm root CA so that none of the nodes trust certificates signed by the old root CA anymore. Run docker swarm ca --rotate to generate a new CA certificate and key. If you prefer, you can pass the --ca-cert and --external-ca flags to specify the root certificate and to use a root CA external to the swarm. Alternately, you can pass the --ca-cert and --ca-key flags to specify the exact certificate and key you would like the swarm to use. Control Plane Security Docker Swarm comes with integrated PKI. All managers and nodes in the Swarm have a cryptographically signed identity in the form of a signed certificate. All manager-to-manager and manager-to-node control communication is secured out of the box with TLS. There is no need to generate certs externally or set up any CAs manually to get end-to-end control plane traffic secured in Docker Swarm mode. Certificates are periodically and automatically rotated. Backup swarm To backup the Swarm on Linux using Docker Engine >= 17.03, you can use the following steps. Since this will need you to stop the engine of a manager, your cluster need to be healthy with at least 3 managers. 1. Select a manager node to do the operation. Try not to choose the leader one in order to avoid a new election inside the cluster. Optional: Store the Docker version to a variable for easy addition to your backup name. Stop the Docker Engine on the manager before backing up the data, so that no data is being changed during the backup. Backup the entire Swarm folder /var/lib/docker/swarm. Restart the manager Docker Engine. https://success.docker.com/article/backup-restore-swarm-manager Docker manager nodes store the swarm state and manager logs in the /var/lib/docker/swarm/ https://docs.docker.com/engine/swarm/admin_guide/#back-up-the-swarm \"/etc\" is used for configurations (.conf files etc). here you find all the configs and settings for your system. \"/var\" is usually used for log files, 'temporary' files (like mail spool, printer spool, etc), databases, and all other data not tied to a specific user. Logs are usually in \"/var/log\", databases in \"/var/lib\" (mysql - \"/var/lib/mysql\"), etc. https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard join worker node The Docker Engine joins the swarm depending on the join-token you provide to the docker swarm join command. To retrieve the join command including the join token for worker nodes, run the following command on a manager node: $ docker swarm join-token worker To add a worker to this swarm, run the following command with the output from the previous command: docker swarm join \\ --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\ 192.168.99.100:2377 This node joined a swarm as a worker. --force-new-cluster This flag forces an existing node that was part of a quorum that was lost to restart as a single node Manager without losing its data. Routing Mesh By default, ports are published using ingress mode. This means that the swarm routing mesh makes the service accessible at the published port on every node regardless if there is a task for the service running on the node. However, if you set host mode explicitly, the port is only bound on nodes where the service is running, and a given port on a node can only be bound once. You can only set the publication mode using the long syntax. You can publish service ports to make them available externally to the swarm using the --publish flag. The --publish flag can take two different styles of arguments. The short version is positional and allows you to specify the published port and target port separated by a colon (:). $ docker service create --name my_web --replicas 3 --publish 8080:80 nginx Here, the published port is 8080 and the target port is of the container is 80. There is also a long format, which is easier to read and allows you to specify more options. $ docker service create --name my_web --replicas 3 --publish published=8080,target=80 nginx Basics Global services provide high availability Draining a node will remove global service Replicated services can be stopped by keeping --replicas to 0 Default swarm mode is replicated Swarm Backup a) Stop Docker Engine on any manager to ensure files are static. b) Copy the /var/lib/docker/swarm directory content for backing up Swarm. - Replicated services c) They can be stopped using the Docker service update: --replicas 0 <SERVICENAME>. d) We will use Go templates to be able to provide unique resources, such as volumes or hostnames, inside containers to ensure all replicas use their own resources. Methods used to publish applications on Docker EE a) We can use Interlock. b) We will publish each application container. c) We can use the host mode to publish applications as if they were running directly at the host level. Which concept is responsible for managing external to internal load balancing for Docker Swarm services? a) Router Mesh Which of the following is true about secrets? a) They are ephemeral and deployed on on-memory filesystems. b) They are encrypted even for administrators, so they cannot be recovered from the control plane. c) If we need to change a secret, we need to create a new secret and update the service with this new one. Encrypt traffic on an overlay network All swarm service management traffic is encrypted by default, using the AES algorithm in GCM mode . Manager nodes in the swarm rotate the key used to encrypt gossip data every 12 hours. To encrypt application data as well, add --opt encrypted when creating the overlay network. This enables IPSEC encryption at the level of the vxlan. This encryption imposes a non-negligible performance penalty, so you should test this option before using it in production. When you enable overlay encryption, Docker creates IPSEC tunnels between all the nodes where tasks are scheduled for services attached to the overlay network. These tunnels also use the AES algorithm in GCM mode and manager nodes automatically rotate the keys every 12 hours. Do not attach Windows nodes to encrypted overlay networks. Overlay network encryption is not supported on Windows. If a Windows node attempts to connect to an encrypted overlay network, no error is detected but the node cannot communicate. generate new CA certificate commands is used to generate a new root CA certificate and root CA key for the swarm cluster? docker swarm ca --rotate","title":"Swarm"},{"location":"Networking/swarm/#docker-swarm","text":"By default, Docker Swarm uses a default address pool 10.0.0.0/8 for global scope (overlay) networks. Every network that does not have a subnet specified will have a subnet sequentially allocated from this pool. In some circumstances it may be desirable to use a different default IP address pool for networks. For example, if the default 10.0.0.0/8 range conflicts with already allocated address space in your network, then it is desirable to ensure that networks use a different range without requiring Swarm users to specify each subnet with the --subnet command. https://docs.docker.com/engine/swarm/swarm-mode/#configuring-default-address-pools Docker Swarm Task states Create a service by using docker service create. The request goes to a Docker manager node. The Docker manager node schedules the service to run on particular nodes. Each service can start multiple tasks. Each task has a life cycle, with states like NEW, PENDING, and COMPLETE. There is no limit on the number of manager nodes. The decision about how many manager nodes to implement is a trade-off between performance and fault-tolerance. Adding manager nodes to a swarm makes the swarm more fault-tolerant. However, additional manager nodes reduce write performance because more nodes must acknowledge proposals to update the swarm state. This means more network round-trip traffic. Raft consensus in swarm mode The reason why Docker swarm mode is using a consensus algorithm is to make sure that all the manager nodes that are in charge of managing and scheduling tasks in the cluster, are storing the same consistent state. Raft tolerates up to (N-1)/2 failures and requires a majority or quorum of (N/2)+1 members to agree on values proposed to the cluster.","title":"Docker Swarm"},{"location":"Networking/swarm/#docker-service-discovery","text":"Docker uses embedded DNS to provide service discovery for containers running on a single Docker engine and tasks running in a Docker swarm. The Docker engine checks if the DNS query belongs to a container or service on each network that the requesting container belongs to. If it does, then the Docker engine looks up the IP address that matches the name of a container, task, or service in a key-value store and returns that IP or service Virtual IP (VIP) back to the requester. Here, all nodes participate in a network called routing mesh. https://success.docker.com/article/ucp-service-discovery-swarm DNS round robin (DNS RR) load balancing is another load balancing option for services (configured with --endpoint-mode dnsrr). In DNS RR mode a VIP is not created for each service. The Docker DNS server resolves a service name to individual container IPs in round robin fashion. https://success.docker.com/article/networking To use an external load balancer without the routing mesh, set --endpoint-mode to dnsrr instead of the default value of vip. In this case, there is not a single virtual IP. Instead, Docker sets up DNS entries for the service such that a DNS query for the service name returns a list of IP addresses, and the client connects directly to one of these. You are responsible for providing the list of IP addresses and ports to your load balancer. See Configure service discovery. https://docs.docker.com/engine/swarm/ingress/#without-the-routing-mesh Services using the routing mesh are running in virtual IP (VIP) mode. Even a service running on each node (by means of the --mode global flag) uses the routing mesh. When using the routing mesh, there is no guarantee about which Docker node services client requests. To bypass the routing mesh, you can start a service using DNS Round Robin (DNSRR) mode, by setting the --endpoint-mode flag to dnsrr. https://docs.docker.com/network/overlay/#bypass-the-routing-mesh-for-a-swarm-service","title":"Docker Service Discovery"},{"location":"Networking/swarm/#routing-mesh-vs-host-mode","text":"When you create a swarm service, you can publish that service\u2019s ports to hosts outside the swarm in two ways: You can rely on the routing mesh. When you publish a service port, the swarm makes the service accessible at the target port on every node, regardless of whether there is a task for the service running on that node or not. This is less complex and is the right choice for many types of services. You can publish a service task\u2019s port directly on the swarm node where that service is running. This feature is available in Docker 1.13 and higher. This bypasses the routing mesh and provides the maximum flexibility, including the ability for you to develop your own routing framework. However, you are responsible for keeping track of where each task is running and routing requests to the tasks, and load-balancing across the nodes. By default, services are using the routing mesh which makes the service accessible at the published port on every swarm node. When a user or process connects to a service, any worker node running a service task may respond. To publish a service\u2019s port directly on the node where it is running, use the mode=host option to the --publish flag. For example: $ docker service create \\ --mode global \\ --publish mode=host,target=80,published=8080 \\ --name=nginx \\ nginx:latest https://docs.docker.com/engine/swarm/services/#publish-ports Concepts By default Docker Swarm uses a default address pool 10.0.0.0/8 for global scope (overlay) networks. If --default-addr-pool-mask-len were unspecified or set explicitly to 24, this would result in 256 /24 networks of the form 10.10.X.0/24. Demote manager node to worker node and docker swarm leave Lock your swam to protect its encryption key You don\u2019t need to unlock the swarm when a new node joins the swarm, because the key is propagated to it over mutual TLS. When Docker restarts, you must unlock the swarm first, using a key encryption key generated by Docker when the swarm was locked. You can rotate this key encryption key at any time. When you rotate the unlock key, keep a record of the old key around for a few minutes, so that if a manager goes down before it gets the new key, it may still be unlocked with the old one. Worker node can join the cluster without the need of autolock key A node is an instance of the Docker engine participating in the swarm. Setting a node to DRAIN does not remove standalone containers from that node, such as those created with docker run, docker-compose up, or the Docker Engine API. A node\u2019s status, including DRAIN, only affects the node\u2019s ability to schedule swarm service workloads. https://docs.docker.com/engine/swarm/swarm-tutorial/drain-node/ DRAIN availability prevents a node from receiving new tasks from the swarm manager. It also means the manager stops tasks running on the node and launches replica tasks on a node with ACTIVE availability. https://docs.docker.com/engine/swarm/swarm-tutorial/drain-node/ Sometimes, such as planned maintenance times, you need to set a node to DRAIN availability. DRAIN availability prevents a node from receiving new tasks from the swarm manager. It also means the manager stops tasks running on the node and launches replica tasks on a node with ACTIVE availability. Docker manager nodes store the swarm state and manager logs in the /var/lib/docker/swarm/ directory. Usually, on Linux distributions: \"/etc\" is used for configurations (.conf files etc). here you find all the configs and settings for your system. \"/var\" is usually used for log files, 'temporary' files (like mail spool, printer spool, etc), databases, and all other data not tied to a specific user. Logs are usually in \"/var/log\", databases in \"/var/lib\" (mysql - \"/var/lib/mysql\"), etc. Ingress When you create a swarm service and do not connect it to a user-defined overlay network, it connects to the ingress network by default. The ingress network is created without the --attachable flag, which means that only swarm services can use it, and not standalone containers. You can connect standalone containers to user-defined overlay networks which are created with the --attachable flag. https://docs.docker.com/network/overlay/#attach-a-standalone-container-to-an-overlay-network The routing mesh allows all the swarm nodes to accept connections on the services published ports. When any swarm node receives traffic destined to the published TCP/UDP port of a running service, it forwards the traffic to the service's VIP using a pre-defined overlay network called ingress. The ingress network behaves similarly to other overlay networks, but its sole purpose is to transport mesh routing traffic from external clients to cluster services. It uses the same VIP-based internal load balancing as described in the previous section. https://success.docker.com/article/ucp-service-discovery-swarm#interlockproxyusageexamples Docker Engine swarm mode makes it easy to publish ports for services to make them available to resources outside the swarm. All nodes participate in an ingress routing mesh. The routing mesh enables each node in the swarm to accept connections on published ports for any service running in the swarm, even if there\u2019s no task running on the node. The routing mesh routes all incoming requests to published ports on available nodes to an active container. https://docs.docker.com/engine/swarm/ingress/","title":"Routing Mesh vs Host Mode"},{"location":"Networking/swarm/#initialise-a-swarm","text":"When you initialize a swarm or join a Docker host to an existing swarm, two new networks are created on that Docker host: - an overlay network called ingress, which handles control and data traffic related to swarm services. When you create a swarm service and do not connect it to a user-defined overlay network, it connects to the ingress network by default. - a bridge network called docker_gwbridge, which connects the individual Docker daemon to the other daemons participating in the swarm.","title":"Initialise a swarm"},{"location":"Networking/swarm/#autolock","text":"Docker 1.13 introduces the ability to protect the mutual TLS encryption key and the key used to encrypt and decrypt secrets, by allowing you to take ownership of these keys and to require manual unlocking of your managers. This feature is called autolock. When Docker restarts, you must unlock the swarm first, using a key encryption key generated by Docker when the swarm was locked. You can rotate this key encryption key at any time. To enable autolock on an existing swarm, set the autolock flag to true. docker swarm update --autolock=true https://docs.docker.com/engine/swarm/swarm_manager_locking/#enable-or-disable-autolock-on-an-existing-swarm","title":"autolock"},{"location":"Networking/swarm/#secrets","text":"Sensitive information to containers running on a Swarm are normally stored in a secret. A secret (usually containing credentials, certificates, and other private information) is provided to service at runtime. The secret is saved in the Raft logs. The Raft logs used by swarm managers are encrypted on disk by default. When Docker restarts, both the TLS key used to encrypt communication among swarm nodes, and the key used to encrypt and decrypt Raft logs on disk, are loaded into each manager node\u2019s memory. Docker 1.13 introduces the ability to protect the mutual TLS encryption key and the key used to encrypt and decrypt Raft logs at rest, by allowing you to take ownership of these keys and to require manual unlocking of your managers. This feature is called autolock. When Docker restarts, you must unlock the swarm first, using a key encryption key generated by Docker when the swarm was locked. You can rotate this key encryption key at any time. When you initialize a new swarm, you can use the --autolock flag to enable autolocking of swarm manager nodes when Docker restarts. docker swarm init --autolock To enable autolock on an existing swarm, set the autolock flag to true. docker swarm update --autolock=true In terms of Docker Swarm services, a secret is a blob of data, such as a password, SSH private key, SSL certificate, or another piece of data that should not be transmitted over a network or stored unencrypted in a Dockerfile or in your application\u2019s source code. https://docs.docker.com/engine/swarm/secrets/#about-secrets The command to create a secret from a file or standard input (STDIN) as content is: docker secret create [OPTIONS] SECRET [file|-] Since this is a cluster management command, it must be executed on a swarm manager node. https://docs.docker.com/engine/reference/commandline/secret_create/ Note: After you create a secret, you cannot update it. You can only remove and re-create it, and you cannot remove a secret that a service is using. https://docs.docker.com/engine/swarm/secrets/#advanced-example-use-secrets-with-a-wordpress-service Secrets are encrypted during transit and at rest in a Docker swarm (The secret is stored in the Raft log, which is encrypted) A Docker secret is a blob of sensitive data that should not be transmitted over a network, such as: - Usernames and passwords - TLS certificates and keys - SSH keys - Other important data such as the name of a database or internal server - Generic strings or binary content (up to 500 kb in size) Secret files with custom targets are not directly bind-mounted into Windows containers, since Windows does not support non-directory file bind-mounts. Instead, secrets for a container are all mounted in C:\\ProgramData\\Docker\\internal\\secrets (an implementation detail which should not be relied upon by applications) within the container. Symbolic links are used to point from there to the desired target of the secret within the container. The default target is C:\\ProgramData\\Docker\\secrets. The location of the mount point within the container defaults to /run/secrets/ in Linux containers, or C:\\ProgramData\\Docker\\secrets in Windows containers. You can also specify a custom location. Add or remove secrets Use the --secret-add or --secret-rm options add or remove a service\u2019s secrets. For example: docker service update \\ --secret-rm mysql_password \\ --secret-add source=mysql_password_v2,target=wp_db_password,mode=0400 \\ wordpress This triggers a rolling restart of the WordPress service and the new secret is used. The following example adds a secret named ssh-2 and removes ssh-1: $ docker service update \\ --secret-add source=ssh-2,target=ssh-2 \\ --secret-rm ssh-1 \\ Myservice","title":"Secrets"},{"location":"Networking/swarm/#docker-swarm-init","text":"When you create a swarm by running docker swarm init, Docker designates itself as a manager node. By default, the manager node generates a new root Certificate Authority (CA) along with a key pair, which are used to secure communications with other nodes that join the swarm. If you prefer, you can specify your own externally-generated root CA, using the --external-ca flag of the docker swarm init command.","title":"docker swarm init"},{"location":"Networking/swarm/#pki","text":"The swarm mode public key infrastructure (PKI) system built into Docker makes it simple to securely deploy a container orchestration system. The nodes in a swarm use mutual Transport Layer Security (TLS) to authenticate, authorize, and encrypt the communications with other nodes in the swarm.","title":"PKI"},{"location":"Networking/swarm/#rotating-the-ca-certificate","text":"In the event that a cluster CA key or a manager node is compromised, you can rotate the swarm root CA so that none of the nodes trust certificates signed by the old root CA anymore. Run docker swarm ca --rotate to generate a new CA certificate and key. If you prefer, you can pass the --ca-cert and --external-ca flags to specify the root certificate and to use a root CA external to the swarm. Alternately, you can pass the --ca-cert and --ca-key flags to specify the exact certificate and key you would like the swarm to use.","title":"Rotating the CA certificate"},{"location":"Networking/swarm/#control-plane-security","text":"Docker Swarm comes with integrated PKI. All managers and nodes in the Swarm have a cryptographically signed identity in the form of a signed certificate. All manager-to-manager and manager-to-node control communication is secured out of the box with TLS. There is no need to generate certs externally or set up any CAs manually to get end-to-end control plane traffic secured in Docker Swarm mode. Certificates are periodically and automatically rotated.","title":"Control Plane Security"},{"location":"Networking/swarm/#backup-swarm","text":"To backup the Swarm on Linux using Docker Engine >= 17.03, you can use the following steps. Since this will need you to stop the engine of a manager, your cluster need to be healthy with at least 3 managers. 1. Select a manager node to do the operation. Try not to choose the leader one in order to avoid a new election inside the cluster. Optional: Store the Docker version to a variable for easy addition to your backup name. Stop the Docker Engine on the manager before backing up the data, so that no data is being changed during the backup. Backup the entire Swarm folder /var/lib/docker/swarm. Restart the manager Docker Engine. https://success.docker.com/article/backup-restore-swarm-manager Docker manager nodes store the swarm state and manager logs in the /var/lib/docker/swarm/ https://docs.docker.com/engine/swarm/admin_guide/#back-up-the-swarm \"/etc\" is used for configurations (.conf files etc). here you find all the configs and settings for your system. \"/var\" is usually used for log files, 'temporary' files (like mail spool, printer spool, etc), databases, and all other data not tied to a specific user. Logs are usually in \"/var/log\", databases in \"/var/lib\" (mysql - \"/var/lib/mysql\"), etc. https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard","title":"Backup swarm"},{"location":"Networking/swarm/#join-worker-node","text":"The Docker Engine joins the swarm depending on the join-token you provide to the docker swarm join command. To retrieve the join command including the join token for worker nodes, run the following command on a manager node: $ docker swarm join-token worker To add a worker to this swarm, run the following command with the output from the previous command: docker swarm join \\ --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\ 192.168.99.100:2377 This node joined a swarm as a worker.","title":"join worker node"},{"location":"Networking/swarm/#-force-new-cluster","text":"This flag forces an existing node that was part of a quorum that was lost to restart as a single node Manager without losing its data.","title":"--force-new-cluster"},{"location":"Networking/swarm/#routing-mesh","text":"By default, ports are published using ingress mode. This means that the swarm routing mesh makes the service accessible at the published port on every node regardless if there is a task for the service running on the node. However, if you set host mode explicitly, the port is only bound on nodes where the service is running, and a given port on a node can only be bound once. You can only set the publication mode using the long syntax. You can publish service ports to make them available externally to the swarm using the --publish flag. The --publish flag can take two different styles of arguments. The short version is positional and allows you to specify the published port and target port separated by a colon (:). $ docker service create --name my_web --replicas 3 --publish 8080:80 nginx Here, the published port is 8080 and the target port is of the container is 80. There is also a long format, which is easier to read and allows you to specify more options. $ docker service create --name my_web --replicas 3 --publish published=8080,target=80 nginx","title":"Routing Mesh"},{"location":"Networking/swarm/#basics","text":"Global services provide high availability Draining a node will remove global service Replicated services can be stopped by keeping --replicas to 0 Default swarm mode is replicated Swarm Backup a) Stop Docker Engine on any manager to ensure files are static. b) Copy the /var/lib/docker/swarm directory content for backing up Swarm. - Replicated services c) They can be stopped using the Docker service update: --replicas 0 <SERVICENAME>. d) We will use Go templates to be able to provide unique resources, such as volumes or hostnames, inside containers to ensure all replicas use their own resources. Methods used to publish applications on Docker EE a) We can use Interlock. b) We will publish each application container. c) We can use the host mode to publish applications as if they were running directly at the host level.","title":"Basics"},{"location":"Networking/swarm/#which-concept-is-responsible-for-managing-external-to-internal-load-balancing-for-docker-swarm-services","text":"a) Router Mesh","title":"Which concept is responsible for managing external to internal load balancing for Docker Swarm services?"},{"location":"Networking/swarm/#which-of-the-following-is-true-about-secrets","text":"a) They are ephemeral and deployed on on-memory filesystems. b) They are encrypted even for administrators, so they cannot be recovered from the control plane. c) If we need to change a secret, we need to create a new secret and update the service with this new one.","title":"Which of the following is true about secrets?"},{"location":"Networking/swarm/#encrypt-traffic-on-an-overlay-network","text":"All swarm service management traffic is encrypted by default, using the AES algorithm in GCM mode . Manager nodes in the swarm rotate the key used to encrypt gossip data every 12 hours. To encrypt application data as well, add --opt encrypted when creating the overlay network. This enables IPSEC encryption at the level of the vxlan. This encryption imposes a non-negligible performance penalty, so you should test this option before using it in production. When you enable overlay encryption, Docker creates IPSEC tunnels between all the nodes where tasks are scheduled for services attached to the overlay network. These tunnels also use the AES algorithm in GCM mode and manager nodes automatically rotate the keys every 12 hours. Do not attach Windows nodes to encrypted overlay networks. Overlay network encryption is not supported on Windows. If a Windows node attempts to connect to an encrypted overlay network, no error is detected but the node cannot communicate.","title":"Encrypt traffic on an overlay network"},{"location":"Networking/swarm/#generate-new-ca-certificate","text":"commands is used to generate a new root CA certificate and root CA key for the swarm cluster? docker swarm ca --rotate","title":"generate new CA certificate"},{"location":"Security/apparmor/","text":"Docker automatically generates and loads a default profile for containers named docker-default. The Docker binary generates this profile in tmpfs and then loads it into the kernel. Note: This profile is used on containers, not on the Docker Daemon. $ docker run --rm -it --security-opt apparmor=docker-default hello-world","title":"AppArmor"},{"location":"Security/seccomp/","text":"seccomp Secure computing mode (seccomp) is a Linux kernel feature. You can use it to restrict the actions available within the container. The seccomp() system call operates on the seccomp state of the calling process. You can use this feature to restrict your application\u2019s access. https://docs.docker.com/engine/security/seccomp/ Seccomp (short for Secure Computing Mode) is a security feature of the Linux kernel, used to restrict the syscalls available to a given process. This facility has been in the kernel in various forms since 2.6.12 and has been available in Docker Engine since 1.10. The current implementation in Docker Engine provides a default set of restricted syscalls and also allows syscalls to be filtered via either a whitelist or a blacklist on a per-container basis (i.e. different filters can be applied to different containers running in the same Engine). Seccomp profiles are applied at container creation time and cannot be altered for running containers. $ grep SECCOMP /boot/config-$(uname -r) Need to have Y for all the flags sudo systemctl cat docker /etc/docker/seccomp.json defaultAction - SCMP_ACT_ERRNO my_seccomp_profile.json { \"defaultAction\": \"SCMP_ACT_ALLOW\", \"architectures\": [ \"SCMP_ARCH_X86_64\", \"SCMP_ARCH_X86\", \"SCMP_ARCH_X32\" ], \"syscalls\": [ { \"name\": \"chmod\", \"action\": \"SCMP_ACT_ERRNO\" }, { \"name\": \"mkdir\", \"action\": \"SCMP_ACT_ERRNO\" } ] } docker run --rm -it --security-opt seccomp=/home/cloud_user/seccomp_stuff/my_seccomp_profile.json debian:jessie sh $ mkdir test # works - it denies $ chmod +x test # Doesn't work - chmod is being called differently in Ubuntu docker run --rm -it --security-opt seccomp=unconfined debian:jessie sh $ touch /home/test $ apt-get update $ apt-get install strace $ strace -qc chmod +x /home/test # fchmodat is the actual call here my_seccomp_profile.json { \"defaultAction\": \"SCMP_ACT_ALLOW\", \"architectures\": [ \"SCMP_ARCH_X86_64\", \"SCMP_ARCH_X86\", \"SCMP_ARCH_X32\" ], \"syscalls\": [ { \"name\": \"chmod\", \"action\": \"SCMP_ACT_ERRNO\" }, { \"name\": \"fchmod\", \"action\": \"SCMP_ACT_ERRNO\" }, { \"name\": \"fchmodat\", \"action\": \"SCMP_ACT_ERRNO\" }, { \"name\": \"mkdir\", \"action\": \"SCMP_ACT_ERRNO\" } ] } docker run --rm -it --security-opt seccomp=/home/cloud_user/seccomp_stuff/my_seccomp_profile.json debian:jessie sh $ mkdir test # works - it denies $ chmod +x test # works - it denies","title":"Seccomp"},{"location":"Security/seccomp/#seccomp","text":"Secure computing mode (seccomp) is a Linux kernel feature. You can use it to restrict the actions available within the container. The seccomp() system call operates on the seccomp state of the calling process. You can use this feature to restrict your application\u2019s access. https://docs.docker.com/engine/security/seccomp/ Seccomp (short for Secure Computing Mode) is a security feature of the Linux kernel, used to restrict the syscalls available to a given process. This facility has been in the kernel in various forms since 2.6.12 and has been available in Docker Engine since 1.10. The current implementation in Docker Engine provides a default set of restricted syscalls and also allows syscalls to be filtered via either a whitelist or a blacklist on a per-container basis (i.e. different filters can be applied to different containers running in the same Engine). Seccomp profiles are applied at container creation time and cannot be altered for running containers. $ grep SECCOMP /boot/config-$(uname -r) Need to have Y for all the flags sudo systemctl cat docker /etc/docker/seccomp.json defaultAction - SCMP_ACT_ERRNO my_seccomp_profile.json { \"defaultAction\": \"SCMP_ACT_ALLOW\", \"architectures\": [ \"SCMP_ARCH_X86_64\", \"SCMP_ARCH_X86\", \"SCMP_ARCH_X32\" ], \"syscalls\": [ { \"name\": \"chmod\", \"action\": \"SCMP_ACT_ERRNO\" }, { \"name\": \"mkdir\", \"action\": \"SCMP_ACT_ERRNO\" } ] } docker run --rm -it --security-opt seccomp=/home/cloud_user/seccomp_stuff/my_seccomp_profile.json debian:jessie sh $ mkdir test # works - it denies $ chmod +x test # Doesn't work - chmod is being called differently in Ubuntu docker run --rm -it --security-opt seccomp=unconfined debian:jessie sh $ touch /home/test $ apt-get update $ apt-get install strace $ strace -qc chmod +x /home/test # fchmodat is the actual call here my_seccomp_profile.json { \"defaultAction\": \"SCMP_ACT_ALLOW\", \"architectures\": [ \"SCMP_ARCH_X86_64\", \"SCMP_ARCH_X86\", \"SCMP_ARCH_X32\" ], \"syscalls\": [ { \"name\": \"chmod\", \"action\": \"SCMP_ACT_ERRNO\" }, { \"name\": \"fchmod\", \"action\": \"SCMP_ACT_ERRNO\" }, { \"name\": \"fchmodat\", \"action\": \"SCMP_ACT_ERRNO\" }, { \"name\": \"mkdir\", \"action\": \"SCMP_ACT_ERRNO\" } ] } docker run --rm -it --security-opt seccomp=/home/cloud_user/seccomp_stuff/my_seccomp_profile.json debian:jessie sh $ mkdir test # works - it denies $ chmod +x test # works - it denies","title":"seccomp"},{"location":"Security/ssl-certificate/","text":"$ systemctl status docker $ yum install epel-release $ sudo yum install certbot python2-certbot-apache sudo certbot certonly --standalone --preferred-challenges http --non-interactive --staple-ocsp --agree-tos -m venkat_lanka@outlook.com -d servername sudo su cd /etc/letsencrypt/live cp private.pem domain.key cat cert.pen chain.pen > domain.crt chmod 777 domain.* sudo mkdir -p /mnt/docker-registry sudo chown cloud_user:cloud_user /mnt/docker-registry sudo docker run --entrypoint htpasswd registry:latest -Bbb testuser testpass > mnt/docker-registry/passfile sudo docker run -d -p 443:5000 --restart=always --name registry -v /etc/letsencrypt/live/myservername:/certs:Z -v /mnt/docker-registry:/var/lib/registry:Z -e REGISTRY_HTTP_TLS_CERTIFICATE: /certs/domain.crt -e REGISTRY_HTTP_TLS_KEY: /certs/domain.key -e REGISTRY_AUTH: htpasswd -e \"REGISTRY_AUTH_HTPASSWD_PATH: /var/lib/registry/passfile\" -e \"REGISTRY_AUTH_HTPASSWD_REALM: Registry Realm\" registry:latest Test the repos using curl curl https://testuser:testpass@myservername:443/v2/_catalog Create private image and put it into repository","title":"SSL Certificate"},{"location":"Storage/Storage/","text":"Storage drivers are also called as Graph drivers. Which driver to use depends on the context (OS and local configuration factors). overlay2 - recent Ubuntu, CentOS and RHEL versions aufs - Ubuntu 14.04 and older devicemapper - CentOS7 and earlier Models: Filesystem storage - overlay2 and aufs - inefficient for heavy workloads Block storage - devicemapper - efficient for heavy workloads Object storage - Flexible and scalable All the data of the images and containers are stored at /var/lib/docker/","title":"Storage"},{"location":"Storage/configure-device-mapper/","text":"Device mapper supports 2 modes : loop-lvm direct-lvm For this lesson, use a CentOS 7 server with a size of Small . Before starting the lesson, you'll first need to install Docker. Add a new storage device to your server. In Playground, select Actions , then select Add /dev/nvme1n1 and wait for it to finish adding the device. Stop and disable Docker. sudo systemctl disable docker sudo systemctl stop docker 4.Delete any existing Docker data. sudo rm -rf /var/lib/docker 5.Configure DeviceMapper in daemon.json . sudo vi /etc/docker/daemon.json { \"storage-driver\": \"devicemapper\", \"storage-opts\": [ \"dm.directlvm_device=/dev/nvme1n1\", \"dm.thinp_percent=95\", \"dm.thinp_metapercent=1\", \"dm.thinp_autoextend_threshold=80\", \"dm.thinp_autoextend_percent=20\", \"dm.directlvm_device_force=true\" ] } 6.Start and enable Docker. sudo systemctl enable docker sudo systemctl start docker 7.Check the storage driver information provided by docker info . docker info 8.Run a container to verify that everything is working. docker run hello-world","title":"Configure Devicemapper"},{"location":"Storage/devicemapper/","text":"The devicemapper driver uses block devices dedicated to Docker and operates at the block level, rather than the file level. With Docker 17.06 and higher, Docker can manage the block device for you, simplifying configuration of direct-lvm mode. This is appropriate for fresh Docker setups only. You can only use a single block device. If you need to use multiple block devices, configure direct-lvm mode manually instead. Device Mapper is a kernel-based framework that underpins many advanced volume management technologies on Linux. Docker\u2019s devicemapper storage driver leverages the thin provisioning and snapshotting capabilities of this Device Mapper framework for image and container management. Particular storage-driver can be configured with options specified with --storage-opt flags. Options for devicemapper are prefixed with dm. Device Mapper configuration modes: loop-lvm mode - is only appropriate for testing. The loop-lvm mode makes use of a \u2018loopback\u2019 mechanism that allows files on the local disk to be read from and written to as if they were an actual physical disk or block device. direct-lvm mode - Production hosts using the devicemapper storage driver must use direct-lvm mode. This is faster than using loopback devices, uses system resources more efficiently, and block devices can grow as needed. However, more setup is required than in loop-lvm mode. The devicemapper driver uses block devices dedicated to Docker and operates at the block level, rather than the file level. With Docker 17.06 and higher, Docker can manage the block device for you, simplifying configuration of direct-lvm mode. This is appropriate for fresh Docker setups only. You can only use a single block device. If you need to use multiple block devices, configure direct-lvm mode manually instead. The vfs storage driver is intended for testing purposes, and for situations where no copy-on-write filesystem can be used. Performance of this storage driver is poor, and is not generally recommended for production use. devicemapper, btrfs, and zfs are block-level storage drivers. The overlay storage driver only works with 2 layers: lowerdir and upperdir.","title":"Devicemapper"},{"location":"Storage/docker-volumes/","text":"When mounting external storage we can either use bind mount or volumes Bind mount: Mount a specific path on the host machine to the container Volumes: Stores data on the host file system, but the storage location is managed by docker More portable Can mount same volume to multiple containers Work in many scenarios 1.Create a directory on the host with some test data. cd ~/ mkdir message echo Hello, world! > message/message.txt 2.Mount the directory to a container with a bind mount. docker run --mount type=bind,source=/home/cloud_user/message,destination=/root,readonly busybox cat /root/message.txt 3.Run a container with a mounted volume. docker run --mount type=volume,source=my-volume,destination=/root busybox sh -c 'echo hello > /root/message.txt' 4.Use the -v syntax to create a bind mount and a volume. docker run -v /home/cloud_user/message:/root:ro busybox cat /root/message.txt docker run -v my-volume:/root busybox sh -c 'cat /root/message.txt' 5.Use a volume to share data between containers. docker run --mount type=volume,source=shared-volume,destination=/root busybox sh -c 'echo I wrote this! > docker run --mount type=volume,source=shared-volume,destination=/anotherplace busybox cat /anotherplace/m 6.Create and manage volumes using docker volume commands. docker volume create test-volume docker volume ls docker volume inspect test-volume docker volume rm test-volume","title":"Volumes Handson"},{"location":"Storage/image-cleanup/","text":"1.Display the storage space being used by Docker. docker system df 2.Display disk usage by individual objects. docker system df -v 3.Delete dangling images (images with no tags or containers). docker image prune 4.Pull an image not being used by any containers, and use docker image prune -a to clean up all images with no containers. docker image pull nginx:1.14.0 docker image prune -a","title":"Image Cleanup"},{"location":"Storage/storage-in-cluster/","text":"Docker volumes provide persistent storage to containers, and they can be shared between containers to allow them to interact with the same data. However, sharing volumes creates additional challenges in the context of a Swarm cluster, where containers may be running on different nodes. In this lesson, we will discuss how you can create shared volumes that work across multiple swarm nodes using the vieux/sshfs volume driver 1.Install the vieux/sshfs plugin on all nodes in the swarm. docker plugin install --grant-all-permissions vieux/sshfs 2.Set up an additional server to use for storage. You can use the Ubuntu 18.04 Bionic Beaver LTS image with a size of Small . On this new storage server, create a directory with a file that can be used for testing. mkdir /home/cloud_user/external echo External storage! > /home/cloud_user/external/message.txt 3.On the swarm manager, manually create a Docker volume that uses the external storage server for storage. Be sure to replace the text and with actual values. docker volume create --driver vieux/sshfs \\ -o sshcmd=cloud_user@<STORAGE_SERVER_PRIVATE_IP>:/home/cloud_user/external \\ -o password=<PASSWORD> \\ sshvolume docker volume ls 4.Create a service that automatically manages the shared volume, creating the volume on swarm nodes as needed. Be sure to replace the text and with actual values. 5.Check the service logs to verify that the service is reading the test data from the external storage server. docker service logs storage-service","title":"Storage in a cluster"},{"location":"Storage/use-storage-volumes-with-docker-swarm/","text":"Introduction Storage volumes provide a powerful and flexible way to add persistent storage to your containers, but what if you need to share storage volumes across multiple Docker hosts, such as a Swarm cluster? In this lab, you will have the opportunity to work with a simple method of creating shared volumes usable across multiple swarm nodes using the sshfs volume driver. Solution 1.Begin by logging in to the storage server using the credentials provided on the hands-on lab page: ssh cloud_user@PUBLIC_IP_ADDRESS Set up the External Storage Location 1.On the storage server, create the storage directory. sudo mkdir -p /etc/docker/storage sudo chown cloud_user:cloud_user /etc/docker/storage 2.Copy the nginx configuration file into the storage directory. cp /home/cloud_user/nginx.conf /etc/docker/storage/ Install the vieux/sshfs Plugin 1.Install the vieux/sshfs plugin on the swarm manager and worker node. docker plugin install --grant-all-permissions vieux/sshfs Create the nginx Service That Uses the Shared Volume 1.Create the nginx-web service on the swarm manager. Create the container. Be sure to replace with the actual password. docker service create -d \\ --replicas=3 \\ --name nginx-web \\ -p 8080:9773 \\ --mount volume-driver=vieux/sshfs,source=nginx-config-vol,target=/etc/nginx/,volume-opt=sshcmd=cloud_user@10.0.1.103:/etc/docker/storage,volume-opt=password=\"<cloud_user password>\" nginx:latest 2.Verify that the service is working properly. curl localhost:8080 If everything is set up correctly, you should see HTML from the nginx Welcome page.","title":"Using Storage Volumes with Docker Swarm"},{"location":"Storage/using-volumes-in-docker-containers/","text":"Introduction Containers are designed to be ephemeral, so when you need persistent data, it is usually not a good idea to store it directly in the container's file system. This is where Docker volumes come into play. Docker volumes allow you to store persistent data outside the container itself, providing greater flexibility in what you can do with your data. In this lab, you will have the opportunity to solve a complex problem using Docker volumes. You will have a chance to work with both shared volumes and bind mounts in order to implement two containers which work together, one container transforming data created by the other. This will give you some practice in working with volumes, as well as some insight into the many ways in which Docker volumes can be used. Solution 1.Begin by logging in to the lab server using the credentials provided on the hands-on lab page: ssh cloud_user@PUBLIC_IP_ADDRESS Create the Shared Volume and Counter Container 1.Create a shared volume. docker volume create test-data 2.Create the counter container with the provided command, and mount the shared volume to the container. docker run --name counter -d \\ --mount type=volume,source=test-data,destination=/var/log/test \\ busybox \\ sh -c 'i=0; while true; do echo \"$i: $(date)\" >> /var/log/test/1.log; i=$((i+1)); sleep 1; done' You can confirm that the counter container is generating data by examining the file inside the container: docker exec counter cat /var/log/test/1.log Create the fluentd Container 1.Create the fluentd container and mount the shared volume, the config file, and the output directory to it. docker run --name fluentd -d \\ --mount type=volume,source=test-data,destination=/var/log/input \\ --mount type=bind,source=/etc/fluentd/fluent.conf,destination=/fluentd/etc/fluent.conf \\ --mount type=bind,source=/etc/fluentd/output,destination=/var/log/output \\ --env FLUENTD_ARGS=\"-c /fluentd/etc/fluent.conf\" \\ k8s.gcr.io/fluentd-gcp:1.30 2.Verify that the fluentd container is generating output on the Docker host. ls /etc/fluentd/output You should see some files containing the transformed log data.","title":"Using Volumes in Docker Containers"},{"location":"Storage/volumes/","text":"docker volume create test-data docker run --name counter -d \\ --mount type=volume,source=test-data,destination=/var/log/test \\ busybox \\ sh -c 'i=0; while true; do echo \"$i: $(date)\" >> /var/log/test/1.log; i=$((i+1)); sleep 1; done' docker exec counter cat /var/log/test/1.log docker run --name fluentd -d \\ --mount type=volume,source=test-data,destination=/var/log/input \\ --mount type=bind,source=/etc/fluentd/fluent.conf,destination=/fluentd/etc/fluent.conf \\ --mount type=bind,source=/etc/fluentd/output,destination=/var/log/output \\ --env FLUENTD_ARGS=\"-c /fluentd/etc/fluent.conf\" \\ k8s.gcr.io/fluentd-gcp:1.30 Bind mounts Bind mounts have been around since the early days of Docker. Bind mounts have limited functionality compared to volumes. When you use a bind mount, a file or directory on the host machine is mounted into a container. The file or directory is referenced by its full or relative path on the host machine. By contrast, when you use a volume, a new directory is created within Docker\u2019s storage directory on the host machine, and Docker manages that directory\u2019s contents. Use the following command to bind-mount the target/ directory into your container at /app/. Run the command from within the source directory. The $(pwd) sub-command expands to the current working directory on Linux or macOS hosts. The --mount and -v examples below produce the same result. You can\u2019t run them both unless you remove the devtest container after running the first one. $ docker run -d \\ -it \\ --name devtest \\ --mount type=bind,source=\"$(pwd)\"/target,target=/app \\ nginx:latest $ docker run -d \\ -it \\ --name devtest \\ -v \"$(pwd)\"/target:/app \\ nginx:latest volumes To mount a volume use -v or --volume. These flags consist of three fields, separated by colon characters (:). The fields must be in the correct order, and the meaning of each field is not immediately obvious. - In the case of named volumes, the first field is the name of the volume, and is unique on a given host machine. For anonymous volumes, the first field is omitted. - The second field is the path where the file or directory are mounted in the container. - The third field is optional, and is a comma-separated list of options, such as ro. These options are discussed below. If you start a container with a volume that does not yet exist, Docker creates the volume for you. The following example mounts the volume myvol2 into /app/ in the container. $ docker run -d \\ --name devtest \\ -v myvol2:/app \\ nginx:latest Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure of the host machine, volumes are completely managed by Docker. Volumes have several advantages over bind mounts: Volumes are easier to back up or migrate than bind mounts. Volumes are useful for backups, restores, and migrations. Use the --volumes-from flag to create a new container that mounts that volume. To remove all unused local volumes use: docker volume prune [OPTIONS] Unused local volumes are those which are not referenced by any containers Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster. To manage volume, you could use: docker volume COMMAND COMMAND You can use subcommands to create, inspect, list, remove, or prune volumes. Create a volume docker volume create Display detailed information on one or more volumes docker volume inspect List volumes docker volume ls Remove all unused local volumes docker volume prune Remove one or more volumes docker volume rm tmpfs As opposed to volumes and bind mounts, a tmpfs mount is temporary, and only persisted in the host memory. When the container stops, the tmpfs mount is removed, and files written there won\u2019t be persisted. This is useful to temporarily store sensitive files that you don\u2019t want to persist in either the host or the container writable layer. volumes-from The docker run command provides a flag, --volumes-from, that will copy the mount definitions from one or more containers to the new container. By combining this flag and volumes, you can build shared-state relationships in a host-independent way. remove volumes Anonymous volumes can be cleaned up in two ways. First, anonymous volumes are automatically deleted when the container they were created for are automatically cleaned up. This happens when containers are deleted via the docker run --rm or docker rm -v flags. Remove one or more containers docker rm [OPTIONS] CONTAINER [CONTAINER...] Second, they can be manually deleted by issuing a docker volume remove command: docker volume rm [OPTIONS] VOLUME [VOLUME...] Which option will bind an already-created DATA volume inside a container, under the /data directory? a) -v DATA:/data b) --mount type=volume,source=DATA,target=/data data representation has no affect Data within the container is exposed as either directory or individual files in container's FS. So, the type of storage mount (volume, bind or tmpfs) does not have any control on data's representation. To differentiate among the storage mounts: volume, bind mount, and tmpfs, user need to check the location where these mounts store container's data on host system. Bind mount can grant read/write permissions on host file system to a process running inside a container. tmpfs is ephemeral and bind mount is less secure compared to volume so if the data to be stored is sensitive such as passwords or credentials, volumes are the ideal Docker CLI with bind mounts You can\u2019t use Docker CLI commands to directly manage bind mounts","title":"Volumes"},{"location":"Storage/volumes/#bind-mounts","text":"Bind mounts have been around since the early days of Docker. Bind mounts have limited functionality compared to volumes. When you use a bind mount, a file or directory on the host machine is mounted into a container. The file or directory is referenced by its full or relative path on the host machine. By contrast, when you use a volume, a new directory is created within Docker\u2019s storage directory on the host machine, and Docker manages that directory\u2019s contents. Use the following command to bind-mount the target/ directory into your container at /app/. Run the command from within the source directory. The $(pwd) sub-command expands to the current working directory on Linux or macOS hosts. The --mount and -v examples below produce the same result. You can\u2019t run them both unless you remove the devtest container after running the first one. $ docker run -d \\ -it \\ --name devtest \\ --mount type=bind,source=\"$(pwd)\"/target,target=/app \\ nginx:latest $ docker run -d \\ -it \\ --name devtest \\ -v \"$(pwd)\"/target:/app \\ nginx:latest","title":"Bind mounts"},{"location":"Storage/volumes/#volumes","text":"To mount a volume use -v or --volume. These flags consist of three fields, separated by colon characters (:). The fields must be in the correct order, and the meaning of each field is not immediately obvious. - In the case of named volumes, the first field is the name of the volume, and is unique on a given host machine. For anonymous volumes, the first field is omitted. - The second field is the path where the file or directory are mounted in the container. - The third field is optional, and is a comma-separated list of options, such as ro. These options are discussed below. If you start a container with a volume that does not yet exist, Docker creates the volume for you. The following example mounts the volume myvol2 into /app/ in the container. $ docker run -d \\ --name devtest \\ -v myvol2:/app \\ nginx:latest Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure of the host machine, volumes are completely managed by Docker. Volumes have several advantages over bind mounts: Volumes are easier to back up or migrate than bind mounts. Volumes are useful for backups, restores, and migrations. Use the --volumes-from flag to create a new container that mounts that volume. To remove all unused local volumes use: docker volume prune [OPTIONS] Unused local volumes are those which are not referenced by any containers Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster. To manage volume, you could use: docker volume COMMAND COMMAND You can use subcommands to create, inspect, list, remove, or prune volumes. Create a volume docker volume create Display detailed information on one or more volumes docker volume inspect List volumes docker volume ls Remove all unused local volumes docker volume prune Remove one or more volumes docker volume rm","title":"volumes"},{"location":"Storage/volumes/#tmpfs","text":"As opposed to volumes and bind mounts, a tmpfs mount is temporary, and only persisted in the host memory. When the container stops, the tmpfs mount is removed, and files written there won\u2019t be persisted. This is useful to temporarily store sensitive files that you don\u2019t want to persist in either the host or the container writable layer.","title":"tmpfs"},{"location":"Storage/volumes/#volumes-from","text":"The docker run command provides a flag, --volumes-from, that will copy the mount definitions from one or more containers to the new container. By combining this flag and volumes, you can build shared-state relationships in a host-independent way.","title":"volumes-from"},{"location":"Storage/volumes/#remove-volumes","text":"Anonymous volumes can be cleaned up in two ways. First, anonymous volumes are automatically deleted when the container they were created for are automatically cleaned up. This happens when containers are deleted via the docker run --rm or docker rm -v flags. Remove one or more containers docker rm [OPTIONS] CONTAINER [CONTAINER...] Second, they can be manually deleted by issuing a docker volume remove command: docker volume rm [OPTIONS] VOLUME [VOLUME...]","title":"remove volumes"},{"location":"Storage/volumes/#which-option-will-bind-an-already-created-data-volume-inside-a-container-under-the-data-directory","text":"a) -v DATA:/data b) --mount type=volume,source=DATA,target=/data","title":"Which option will bind an already-created DATA volume inside a container, under the /data directory?"},{"location":"Storage/volumes/#data-representation-has-no-affect","text":"Data within the container is exposed as either directory or individual files in container's FS. So, the type of storage mount (volume, bind or tmpfs) does not have any control on data's representation. To differentiate among the storage mounts: volume, bind mount, and tmpfs, user need to check the location where these mounts store container's data on host system. Bind mount can grant read/write permissions on host file system to a process running inside a container. tmpfs is ephemeral and bind mount is less secure compared to volume so if the data to be stored is sensitive such as passwords or credentials, volumes are the ideal","title":"data representation has no affect"},{"location":"Storage/volumes/#docker-cli-with-bind-mounts","text":"You can\u2019t use Docker CLI commands to directly manage bind mounts","title":"Docker CLI with bind mounts"}]}